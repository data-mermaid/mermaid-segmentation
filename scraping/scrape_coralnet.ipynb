{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbb1971",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d08c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import argparse\n",
    "import traceback\n",
    "import getpass\n",
    "import json\n",
    "from typing import List, Optional, Tuple\n",
    "import concurrent\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import io\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f3c8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoralNetDownloader:\n",
    "    \"\"\"Main downloader class for CoralNet sources using requests\"\"\"\n",
    "    \n",
    "    CORALNET_URL = \"https://coralnet.ucsd.edu\"\n",
    "    LOGIN_URL = \"https://coralnet.ucsd.edu/accounts/login/\"\n",
    "    \n",
    "    def __init__(self, username: str, password: str):\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        })\n",
    "        self.logged_in = False\n",
    "    \n",
    "    def login(self) -> bool:\n",
    "        \"\"\"Log in to CoralNet using requests session\"\"\"\n",
    "        success = False\n",
    "        try:\n",
    "            # Get login page to extract CSRF token\n",
    "            response = self.session.get(self.LOGIN_URL, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            csrf_token = soup.find(\"input\", attrs={\"name\": \"csrfmiddlewaretoken\"})\n",
    "            \n",
    "            if not csrf_token:\n",
    "                raise Exception(\"Could not find CSRF token\")\n",
    "            \n",
    "            # Prepare login data\n",
    "            data = {\n",
    "                \"username\": self.username,\n",
    "                \"password\": self.password,\n",
    "                \"csrfmiddlewaretoken\": csrf_token[\"value\"],\n",
    "            }\n",
    "            \n",
    "            headers = {\"Referer\": self.LOGIN_URL}\n",
    "            \n",
    "            # Submit login\n",
    "            login_response = self.session.post(\n",
    "                self.LOGIN_URL, \n",
    "                data=data, \n",
    "                headers=headers,\n",
    "                timeout=30,\n",
    "                allow_redirects=True\n",
    "            )\n",
    "            \n",
    "            # Check if login was successful by looking for sign out button or redirect\n",
    "            if \"Sign out\" in login_response.text or login_response.url != self.LOGIN_URL:\n",
    "                success = True\n",
    "                self.logged_in = True\n",
    "                print(\"✓ Login successful\")\n",
    "            else:\n",
    "                raise Exception(\"Login failed - invalid credentials or other error\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not login with {self.username}: {str(e)}\")\n",
    "        \n",
    "        return success\n",
    "    \n",
    "    def check_permissions(self, source_id: int) -> bool:\n",
    "        \"\"\"Check permissions for accessing a source\"\"\"\n",
    "        try:\n",
    "            url = f\"{self.CORALNET_URL}/source/{source_id}/\"\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            if \"Page could not be found\" in response.text:\n",
    "                raise Exception(\"Source does not exist\")\n",
    "            elif \"don't have permission\" in response.text:\n",
    "                raise Exception(\"Permission denied\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Permission check failed for source {source_id}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def download_metadata(self, source_id: int, output_dir: str) -> Tuple[bool, int]:\n",
    "        \"\"\"Download metadata for a source\"\"\"\n",
    "        success = False\n",
    "        total_images_number = 0\n",
    "        \n",
    "        try:\n",
    "            url = f\"{self.CORALNET_URL}/source/{source_id}/\"\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Try to get total images count\n",
    "            try:\n",
    "                image_status_header = soup.find(\"h4\", string=\"Image Status\")\n",
    "                if image_status_header:\n",
    "                    table = image_status_header.find_next_sibling(\"table\", class_=\"detail_box_table\")\n",
    "                    if table:\n",
    "                        # Find the row where one of the <td> contains 'Total images:'\n",
    "                        total_images_row = None\n",
    "                        for tr in table.find_all(\"tr\"):\n",
    "                            tds = tr.find_all(\"td\")\n",
    "                            if any(\"Total images:\" in td.get_text() for td in tds):\n",
    "                                total_images_row = tr\n",
    "                                break\n",
    "                        if total_images_row:\n",
    "                            link = total_images_row.find(\"a\")\n",
    "                            if link:\n",
    "                                try:\n",
    "                                    total_images_number = int(link.get_text().strip().replace(\",\", \"\"))\n",
    "                                except Exception:\n",
    "                                    total_images_number = 0\n",
    "                                print(f\"Total images: {total_images_number}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Can't get number of images: {e}\")\n",
    "                total_images_number = 0\n",
    "            \n",
    "            # Extract classifier plot data from JavaScript\n",
    "            script_tags = soup.find_all(\"script\")\n",
    "            classifier_data = None\n",
    "            \n",
    "            for script in script_tags:\n",
    "                if script.string and \"Classifier overview\" in script.string:\n",
    "                    script_text = script.string\n",
    "                    start_marker = \"let classifierPlotData = \"\n",
    "                    start_index = script_text.find(start_marker)\n",
    "                    \n",
    "                    if start_index != -1:\n",
    "                        start_index += len(start_marker)\n",
    "                        end_index = script_text.find(\"];\", start_index) + 1\n",
    "                        classifier_plot_data_str = script_text[start_index:end_index]\n",
    "                        \n",
    "                        # Clean up JavaScript object notation to valid JSON\n",
    "                        classifier_plot_data_str = classifier_plot_data_str.replace(\"'\", '\"')\n",
    "                        \n",
    "                        try:\n",
    "                            classifier_data = json.loads(classifier_plot_data_str)\n",
    "                            break\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Warning: Could not parse classifier data: {e}\")\n",
    "            \n",
    "            if not classifier_data:\n",
    "                print(\"No metadata found for this source\")\n",
    "                return True, total_images_number\n",
    "            \n",
    "            # Process classifier data\n",
    "            meta = []\n",
    "            for point in classifier_data:\n",
    "                meta.append([\n",
    "                    point.get(\"x\"),        # classifier_nbr\n",
    "                    point.get(\"y\"),        # score\n",
    "                    point.get(\"nimages\"),  # nimages\n",
    "                    point.get(\"traintime\"), # traintime\n",
    "                    point.get(\"date\"),     # date\n",
    "                    point.get(\"pk\")        # src_id\n",
    "                ])\n",
    "            \n",
    "            # Save metadata\n",
    "            meta_df = pd.DataFrame(meta, columns=[\n",
    "                'Classifier nbr', 'Accuracy', 'Trained on',\n",
    "                'Date', 'Traintime', 'Global id'\n",
    "            ])\n",
    "            print(meta_df)\n",
    "            filepath = os.path.join(output_dir, \"metadata.csv\")\n",
    "            meta_df.to_csv(filepath, index=False)\n",
    "            \n",
    "            if os.path.exists(filepath):\n",
    "                print(f\"✓ Metadata saved to {filepath}\")\n",
    "                success = True\n",
    "            else:\n",
    "                raise Exception(\"Metadata could not be saved\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Issue downloading metadata: {str(e)}\")\n",
    "        \n",
    "        return success, total_images_number\n",
    "    \n",
    "    def download_labelset(self, source_id: int, output_dir: str) -> bool:\n",
    "        \"\"\"Download labelset for a source\"\"\"\n",
    "        success = False\n",
    "        \n",
    "        try:\n",
    "            url = f\"{self.CORALNET_URL}/source/{source_id}/labelset/\"\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            table = soup.find('table', {'id': 'label-table'})\n",
    "            \n",
    "            if table is None:\n",
    "                raise Exception(\"Unable to find the label table\")\n",
    "            \n",
    "            rows = table.find_all('tr')\n",
    "            if not rows or len(rows) <= 1:  # Only header row or no rows\n",
    "                print(\"No labelset found for this source\")\n",
    "                return True\n",
    "            \n",
    "            label_ids = []\n",
    "            names = []\n",
    "            short_codes = []\n",
    "            \n",
    "            for row in rows[1:]:  # Skip header row\n",
    "                cells = row.find_all('td')\n",
    "                if cells:\n",
    "                    # Get label ID from link\n",
    "                    link = cells[0].find('a')\n",
    "                    if link and link.get('href'):\n",
    "                        label_id = link['href'].split('/')[-2]\n",
    "                        label_ids.append(label_id)\n",
    "                        \n",
    "                        # Get name\n",
    "                        names.append(link.get_text().strip())\n",
    "                        \n",
    "                        # Get short code (second column)\n",
    "                        if len(cells) > 1:\n",
    "                            short_codes.append(cells[1].get_text().strip())\n",
    "                        else:\n",
    "                            short_codes.append(\"\")\n",
    "            \n",
    "            if label_ids:\n",
    "                labelset_df = pd.DataFrame({\n",
    "                    'Label ID': label_ids,\n",
    "                    'Name': names,\n",
    "                    'Short Code': short_codes\n",
    "                })\n",
    "                \n",
    "                filepath = os.path.join(output_dir, \"labelset.csv\")\n",
    "                labelset_df.to_csv(filepath, index=False)\n",
    "                \n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"✓ Labelset saved to {filepath}\")\n",
    "                    success = True\n",
    "                else:\n",
    "                    raise Exception(\"Labelset could not be saved\")\n",
    "            else:\n",
    "                print(\"No labels found in labelset\")\n",
    "                success = True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Issue downloading labelset: {str(e)}\")\n",
    "        \n",
    "        return success\n",
    "    \n",
    "    def download_annotations(self, source_id: int, output_dir: str, n_images: int) -> bool:\n",
    "        \"\"\"Download annotations for a source\"\"\"\n",
    "        success = False\n",
    "        \n",
    "        try:\n",
    "            # First, get the browse images page to extract form data\n",
    "            browse_url = f\"{self.CORALNET_URL}/source/{source_id}/browse/images/\"\n",
    "            response = self.session.get(browse_url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find the export form\n",
    "            export_form = soup.find('form', {'id': 'export-annotations-prep-form'})\n",
    "            if not export_form:\n",
    "                raise Exception(\"Could not find export annotations form\")\n",
    "            \n",
    "            # Extract CSRF token\n",
    "            csrf_token = export_form.find('input', {'name': 'csrfmiddlewaretoken'})\n",
    "            if not csrf_token:\n",
    "                raise Exception(\"Could not find CSRF token in export form\")\n",
    "            \n",
    "            # Prepare form data for annotation export\n",
    "            form_data = {\n",
    "                'csrfmiddlewaretoken': csrf_token['value'],\n",
    "                'browse_action': 'export_annotations',\n",
    "                'image_select_type': 'all',\n",
    "                'label_format': 'both',\n",
    "                # Add all optional columns\n",
    "                'optional_columns': [\n",
    "                    'annotator_info',\n",
    "                    \"metadata_date_aux\",\n",
    "                    \"metadata_other\",\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            export_request_url = f\"{self.CORALNET_URL}/source/{source_id}/annotation/export_prep/\"\n",
    "            # Submit the export request\n",
    "            export_response = self.session.post(\n",
    "                export_request_url,\n",
    "                headers={'Referer': browse_url},\n",
    "                data=form_data,\n",
    "                timeout=120,  # Longer timeout for processing\n",
    "                allow_redirects=True\n",
    "            )\n",
    "            \n",
    "            export_timestamp = export_response.json()['session_data_timestamp']\n",
    "            download_annotations_url = f\"https://coralnet.ucsd.edu/source/{source_id}/export/serve/?session_data_timestamp={export_timestamp}\"\n",
    "            download_annotations_response = self.session.get(download_annotations_url, timeout=60)\n",
    "            download_annotations_response.raise_for_status()\n",
    "\n",
    "            df_annotations = pd.read_csv(io.StringIO(download_annotations_response.text))\n",
    "            annotations_file = os.path.join(output_dir, \"annotations.csv\")\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            df_annotations.to_csv(annotations_file, index=False)\n",
    "            if os.path.exists(annotations_file) and os.path.getsize(annotations_file) > 0:\n",
    "                print(f\"✓ Annotations saved to {annotations_file}\")\n",
    "                success = True\n",
    "            else:\n",
    "                raise Exception(\"Downloaded annotations file is empty\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Issue downloading annotations: {str(e)}\")\n",
    "            success = False  # Don't fail the entire process\n",
    "        \n",
    "        return success\n",
    "    \n",
    "    def get_images_on_page(self, browse_url) -> tuple[dict[str, str], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Get a dictionary of image names and their URLs from the CoralNet browse page\n",
    "        \n",
    "        Args:\n",
    "            session: requests.Session object with valid CoralNet login\n",
    "            browse_url: URL of the browse images page\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with image names as keys and their URLs as values\n",
    "        \"\"\"\n",
    "        images = {}\n",
    "        \n",
    "        response = self.session.get(browse_url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        thumb_wrappers = soup.find_all('span', class_='thumb_wrapper')\n",
    "        for wrapper in thumb_wrappers:\n",
    "            link = wrapper.find('a')\n",
    "            img = wrapper.find('img')\n",
    "            if link and img:\n",
    "                image_name = img.get('title', '')\n",
    "                image_url = link.get('href', '')\n",
    "                \n",
    "                if image_name and image_url:\n",
    "                    images[image_name] = image_url\n",
    "\n",
    "        next_page_element = soup.find('a', title='Next page')\n",
    "        next_page_url = next_page_element.get('href') if next_page_element else None\n",
    "                    \n",
    "        return images, next_page_url\n",
    "\n",
    "    def get_images(self, source_id) -> Tuple[Optional[pd.DataFrame], bool]:\n",
    "        \"\"\"\n",
    "        Get a DataFrame of all images from a CoralNet source\n",
    "        \n",
    "        Args:\n",
    "            session: requests.Session object with valid CoralNet login\n",
    "            source_id: ID of the CoralNet source\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing image names and URLs\n",
    "        \"\"\"\n",
    "        images = None\n",
    "        success = False\n",
    "\n",
    "        base_url = f'{self.CORALNET_URL}/source/{source_id}/browse/images'\n",
    "        all_images = {}\n",
    "        try: \n",
    "            imgs, next_page = self.get_images_on_page(base_url)\n",
    "            all_images.update(imgs)\n",
    "            p_bar = tqdm(desc=\"Fetching images\", unit=\"page\")\n",
    "            while next_page:\n",
    "                imgs, next_page = self.get_images_on_page(f\"{base_url}/{next_page}\")\n",
    "                all_images.update(imgs)\n",
    "                p_bar.update(1)\n",
    "            p_bar.close()\n",
    "            success = True\n",
    "            images = pd.DataFrame(list(all_images.items()), columns=['Name', 'Image Page'])\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Issue retrieving images: {str(e)}\")\n",
    "        return images, success\n",
    "\n",
    "    # def get_images(self, source_id: int) -> Tuple[Optional[pd.DataFrame], bool]:\n",
    "        \"\"\"Get list of images from a source\"\"\"\n",
    "        images = None\n",
    "        success = False\n",
    "        \n",
    "        try:\n",
    "            base_url = f\"{self.CORALNET_URL}/source/{source_id}/browse/images/\"\n",
    "            \n",
    "            # Get first page to determine total pages\n",
    "            response = self.session.get(base_url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find total images count\n",
    "            line_div = soup.find('div', class_='line')\n",
    "            if line_div:\n",
    "                line_text = line_div.get_text()\n",
    "                # Extract total from text like \"Showing 1 to 20 of 150 images\"\n",
    "                parts = line_text.split()\n",
    "                if 'of' in parts:\n",
    "                    total_images = int(parts[parts.index('of') + 1])\n",
    "                    total_pages = (total_images + 19) // 20  # Round up, 20 images per page\n",
    "                    print(f\"Found {total_images} images across {total_pages} pages\")\n",
    "                else:\n",
    "                    total_images = 20  # Assume at least one page\n",
    "                    total_pages = 1\n",
    "            else:\n",
    "                total_images = 20\n",
    "                total_pages = 1\n",
    "            \n",
    "            image_page_urls = []\n",
    "            image_names = []\n",
    "            \n",
    "            # Collect images from all pages\n",
    "            for page in range(1, total_pages + 1):\n",
    "                time.sleep(1)  # Rate limiting\n",
    "                \n",
    "                if page > 1:\n",
    "                    page_url = f\"{base_url}?page={page}\"\n",
    "                else:\n",
    "                    page_url = base_url\n",
    "                \n",
    "                try:\n",
    "                    page_response = self.session.get(page_url, timeout=30)\n",
    "                    page_response.raise_for_status()\n",
    "                    \n",
    "                    page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
    "                    \n",
    "                    # Find thumbnail links and images\n",
    "                    thumb_wrappers = page_soup.find_all('div', class_='thumb_wrapper')\n",
    "                    \n",
    "                    for wrapper in thumb_wrappers:\n",
    "                        link = wrapper.find('a')\n",
    "                        img = wrapper.find('img')\n",
    "                        \n",
    "                        if link and img:\n",
    "                            image_page_urls.append(urllib.parse.urljoin(self.CORALNET_URL, link['href']))\n",
    "                            image_names.append(img.get('alt', ''))\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to get page {page}: {e}\")\n",
    "                    time.sleep(5)  # Wait longer on error\n",
    "                    continue\n",
    "            \n",
    "            if image_names and image_page_urls:\n",
    "                images = pd.DataFrame({\n",
    "                    'Name': image_names,\n",
    "                    'Image Page': image_page_urls\n",
    "                })\n",
    "                print(f\"✓ Found {len(images)} images\")\n",
    "                success = True\n",
    "            else:\n",
    "                print(\"No images found\")\n",
    "                success = True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Issue retrieving images: {str(e)}\")\n",
    "        \n",
    "        return images, success\n",
    "\n",
    "    def get_image_urls(self, image_page_urls: List[str]) -> List[Optional[str]]:\n",
    "        \"\"\"\n",
    "        Get the direct image URLs from the CoralNet image page URLs\n",
    "\n",
    "        Args:\n",
    "            image_page_url: URL of the CoralNet image page\n",
    "\n",
    "        Returns:\n",
    "            str or None: Direct image URL or None if not found\n",
    "        \"\"\"\n",
    "        image_urls = []\n",
    "        for image_page_url in tqdm(image_page_urls, desc=\"Fetching image URLs\", unit=\"image\"):\n",
    "            image_page_url = f'https://coralnet.ucsd.edu{image_page_url}'\n",
    "            image_view_response = urllib.request.urlopen(image_page_url)\n",
    "            response_soup = BeautifulSoup(\n",
    "                image_view_response.read(), 'html.parser')\n",
    "\n",
    "            original_img_elements = response_soup.select(\n",
    "                'div#original_image_container > img')\n",
    "            if not original_img_elements:\n",
    "                raise ValueError(\n",
    "                    f\"CoralNet image {image_page_url}: couldn't find image on the\"\n",
    "                    f\" image-view page. Maybe it's in a private source.\")\n",
    "            image_url = original_img_elements[0].attrs.get('src')\n",
    "            image_urls.append(image_url)\n",
    "\n",
    "        return image_urls\n",
    "    \n",
    "    @staticmethod\n",
    "    def download_image(url: str, path: str, timeout: int = 30) -> Tuple[str, bool]:\n",
    "        \"\"\"Download a single image\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            return path, True\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            \n",
    "            with open(path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            \n",
    "            if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "                return path, True\n",
    "            else:\n",
    "                return path, False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to download {url}: {e}\")\n",
    "            return path, False\n",
    "    \n",
    "    def download_images(self, images_df: pd.DataFrame, output_dir: str, source_id: int):\n",
    "        \"\"\"Download all images from a DataFrame\"\"\"\n",
    "        # Save image list\n",
    "        csv_file = os.path.join(output_dir, \"images.csv\")\n",
    "        images_df.to_csv(csv_file, index=False)\n",
    "        print(f\"✓ Saved image list to {csv_file}\")\n",
    "        \n",
    "        # Create images directory\n",
    "        image_dir = os.path.join(output_dir, \"images\")\n",
    "        os.makedirs(image_dir, exist_ok=True)\n",
    "        \n",
    "        # Filter out rows without URLs\n",
    "        valid_images = images_df[images_df['Image URL'].notna()]\n",
    "        \n",
    "        if valid_images.empty:\n",
    "            print(\"Warning: No valid image URLs found\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Downloading {len(valid_images)} images...\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=min(8, os.cpu_count() or 4)) as executor:\n",
    "            futures = []\n",
    "            for _, row in valid_images.iterrows():\n",
    "                name = row['Name']\n",
    "                url = row['Image URL']\n",
    "                path = os.path.join(image_dir, name)\n",
    "                futures.append(executor.submit(self.download_image, url, path))\n",
    "            \n",
    "            completed = 0\n",
    "            successful = 0\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    path, success = future.result()\n",
    "                    if success:\n",
    "                        successful += 1\n",
    "                    else:\n",
    "                        print(f\"Warning: Failed to download {os.path.basename(path)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: {str(e)}\")\n",
    "                \n",
    "                completed += 1\n",
    "                if completed % 10 == 0 or completed == len(futures):\n",
    "                    print(f\"Progress: {completed}/{len(futures)} images processed\")\n",
    "        \n",
    "        print(f\"✓ Downloaded {successful}/{len(valid_images)} images to {image_dir}\")\n",
    "    \n",
    "    def download_source(self, source_id: int, output_dir: str,\n",
    "                       download_metadata: bool = True,\n",
    "                       download_labelset: bool = True,\n",
    "                       download_annotations: bool = True,\n",
    "                       download_images: bool = True) -> bool:\n",
    "        \"\"\"Download all data for a source\"\"\"\n",
    "        print(f\"\\n=== Downloading Source {source_id} ===\")\n",
    "        \n",
    "        # Create source directory\n",
    "        source_dir = os.path.join(output_dir, str(source_id))\n",
    "        os.makedirs(source_dir, exist_ok=True)\n",
    "        \n",
    "        # Login if needed\n",
    "        if not self.logged_in:\n",
    "            if not self.login():\n",
    "                raise Exception(\"Failed to login to CoralNet\")\n",
    "        \n",
    "        # Check permissions\n",
    "        if not self.check_permissions(source_id):\n",
    "            raise Exception(f\"Cannot access source {source_id}\")\n",
    "        \n",
    "        success = True\n",
    "        n_images = 0\n",
    "        \n",
    "        # Download metadata\n",
    "        if download_metadata:\n",
    "            metadata_success, n_images = self.download_metadata(source_id, source_dir)\n",
    "            if not metadata_success:\n",
    "                print(\"Warning: Failed to download metadata\")\n",
    "            \n",
    "            if n_images == 0:\n",
    "                print(\"Source appears to be empty, creating empty marker\")\n",
    "                os.makedirs(os.path.join(source_dir, \"empty\"), exist_ok=True)\n",
    "                return True\n",
    "        \n",
    "        # Download labelset\n",
    "        if download_labelset:\n",
    "            if not self.download_labelset(source_id, source_dir):\n",
    "                print(\"Warning: Failed to download labelset\")\n",
    "        \n",
    "        # Download annotations\n",
    "        if download_annotations:\n",
    "            if not self.download_annotations(source_id, source_dir, n_images):\n",
    "                print(\"Warning: Failed to download annotations\")\n",
    "        \n",
    "        # Download images\n",
    "        if download_images:\n",
    "            images_df, images_success = self.get_images(source_id)\n",
    "            if images_success and images_df is not None and len(images_df) > 0:\n",
    "                # Get image URLs\n",
    "                image_urls = self.get_image_urls(images_df['Image Page'].tolist())\n",
    "                images_df['Image URL'] = image_urls\n",
    "                \n",
    "                # Download images\n",
    "                self.download_images(images_df, source_dir, source_id)\n",
    "            else:\n",
    "                print(\"Warning: No images found or failed to retrieve image list\")\n",
    "        \n",
    "        print(f\"✓ Completed downloading source {source_id}\")\n",
    "        return success\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        if self.session:\n",
    "            self.session.close()\n",
    "        self.logged_in = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42c77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#         description=\"Download data from CoralNet sources using requests\",\n",
    "#         formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "#         epilog=\"\"\"\n",
    "# Examples:\n",
    "#   %(prog)s 123 -o ./downloads\n",
    "#   %(prog)s 123,456,789 -o ./data --no-images\n",
    "#   %(prog)s 123 -o ./output --metadata-only\n",
    "#   %(prog)s 123 -u myuser -p mypass -o ./downloads\n",
    "# \"\"\"\n",
    "#     )\n",
    "    \n",
    "#     parser.add_argument('source_ids',\n",
    "#                        help='Comma-separated list of source IDs to download')\n",
    "#     parser.add_argument('-o', '--output', required=True,\n",
    "#                        help='Output directory for downloads')\n",
    "#     parser.add_argument('-u', '--username',\n",
    "#                        help='CoralNet username (if not provided, will prompt)')\n",
    "#     parser.add_argument('-p', '--password',\n",
    "#                        help='CoralNet password (if not provided, will prompt)')\n",
    "    \n",
    "#     # Download options\n",
    "#     parser.add_argument('--no-metadata', action='store_true',\n",
    "#                        help='Skip metadata download')\n",
    "#     parser.add_argument('--no-labelset', action='store_true',\n",
    "#                        help='Skip labelset download')\n",
    "#     parser.add_argument('--no-annotations', action='store_true',\n",
    "#                        help='Skip annotations download')\n",
    "#     parser.add_argument('--no-images', action='store_true',\n",
    "#                        help='Skip images download')\n",
    "    \n",
    "#     # Convenience flags\n",
    "#     parser.add_argument('--metadata-only', action='store_true',\n",
    "#                        help='Download only metadata')\n",
    "#     parser.add_argument('--annotations-only', action='store_true',\n",
    "#                        help='Download only annotations')\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     # Parse source IDs\n",
    "#     try:\n",
    "#         source_ids = [int(s.strip()) for s in args.source_ids.split(',')]\n",
    "#     except ValueError:\n",
    "#         print(\"ERROR: Source IDs must be comma-separated integers\")\n",
    "#         sys.exit(1)\n",
    "    \n",
    "#     # Get credentials\n",
    "#     username = \"ViktorDo\" #args.username or input(\"CoralNet username: \")\n",
    "#     password = \"ThACU74QW7iEV@F\" #args.password or getpass.getpass(\"CoralNet password: \")\n",
    "    \n",
    "#     if not username or not password:\n",
    "#         print(\"ERROR: Username and password are required\")\n",
    "#         sys.exit(1)\n",
    "    \n",
    "#     # Set download options\n",
    "#     if args.metadata_only:\n",
    "#         download_metadata = True\n",
    "#         download_labelset = False\n",
    "#         download_annotations = False\n",
    "#         download_images = False\n",
    "#     elif args.annotations_only:\n",
    "#         download_metadata = False\n",
    "#         download_labelset = False\n",
    "#         download_annotations = True\n",
    "#         download_images = False\n",
    "#     else:\n",
    "#         download_metadata = not args.no_metadata\n",
    "#         download_labelset = not args.no_labelset\n",
    "#         download_annotations = not args.no_annotations\n",
    "#         download_images = not args.no_images\n",
    "    \n",
    "#     # Create output directory\n",
    "#     output_dir = os.path.abspath(args.output)\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     print(f\"CoralNet CLI Downloader (Requests Version)\")\n",
    "#     print(f\"Output directory: {output_dir}\")\n",
    "#     print(f\"Source IDs: {source_ids}\")\n",
    "#     print(f\"Options: metadata={download_metadata}, labelset={download_labelset}, \"\n",
    "#           f\"annotations={download_annotations}, images={download_images}\")\n",
    "    \n",
    "#     # Initialize downloader\n",
    "#     downloader = CoralNetDownloader(username=username, password=password)\n",
    "    \n",
    "#     try:\n",
    "#         # Download each source\n",
    "#         for source_id in source_ids:\n",
    "#             try:\n",
    "#                 downloader.download_source(\n",
    "#                     source_id=source_id,\n",
    "#                     output_dir=output_dir,\n",
    "#                     download_metadata=download_metadata,\n",
    "#                     download_labelset=download_labelset,\n",
    "#                     download_annotations=download_annotations,\n",
    "#                     download_images=download_images\n",
    "#                 )\n",
    "#             except Exception as e:\n",
    "#                 print(f\"ERROR: Failed to download source {source_id}: {str(e)}\")\n",
    "#                 traceback.print_exc()\n",
    "#                 continue\n",
    "        \n",
    "#         print(\"\\n=== Download Complete ===\")\n",
    "        \n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"\\nDownload interrupted by user\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"ERROR: {str(e)}\")\n",
    "#         traceback.print_exc()\n",
    "#         sys.exit(1)\n",
    "#     finally:\n",
    "#         downloader.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68a2fb",
   "metadata": {},
   "source": [
    "# Run Model Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6878472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get credentials\n",
    "username = input(\"CoralNet username: \")\n",
    "password = getpass.getpass(\"CoralNet password: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82c9f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id = \"5027\"\n",
    "output_dir = \"./scrape-res\"\n",
    "source_dir = os.path.join(output_dir, str(source_id))\n",
    "os.makedirs(source_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a2d49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Login successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downloader = CoralNetDownloader(username=username, password=password)\n",
    "downloader.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "630b6095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 40\n",
      "   Classifier nbr  Accuracy  Trained on     Date     Traintime Global id\n",
      "0               1        69          22  0:00:02  May 18, 2024     44953\n",
      "1               2        80          35  0:00:03  May 18, 2024     44955\n",
      "✓ Metadata saved to ./scrape-res/5027/metadata.csv\n",
      "✓ Labelset saved to ./scrape-res/5027/labelset.csv\n",
      "✓ Annotations saved to ./scrape-res/5027/annotations.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_success, n_images = downloader.download_metadata(source_id, source_dir)\n",
    "downloader.download_labelset(source_id, source_dir)\n",
    "downloader.download_annotations(source_id, source_dir, n_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e0c313e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4932d428d494b16a8c9f3627cf41f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching images: 0page [00:00, ?page/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e42227afef343c8a13e427112539cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching image URLs:   0%|          | 0/40 [00:00<?, ?image/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved image list to ./scrape-res/5027/images.csv\n",
      "Downloading 40 images...\n",
      "Progress: 10/40 images processed\n",
      "Progress: 20/40 images processed\n",
      "Progress: 30/40 images processed\n",
      "Progress: 40/40 images processed\n",
      "✓ Downloaded 40/40 images to ./scrape-res/5027/images\n"
     ]
    }
   ],
   "source": [
    "images_df, images_success = downloader.get_images(source_id)\n",
    "if images_success and images_df is not None and len(images_df) > 0:\n",
    "    # Get image URLs\n",
    "    image_urls = downloader.get_image_urls(images_df['Image Page'].tolist())\n",
    "    images_df['Image URL'] = image_urls\n",
    "\n",
    "    downloader.download_images(images_df, source_dir, source_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
