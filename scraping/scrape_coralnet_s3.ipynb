{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbb1971",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d08c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f3c8000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoralNetDownloader:\n",
    "    \"\"\"Main downloader class for CoralNet sources using requests\"\"\"\n",
    "    \n",
    "    CORALNET_URL = \"https://coralnet.ucsd.edu\"\n",
    "    LOGIN_URL = \"https://coralnet.ucsd.edu/accounts/login/\"\n",
    "    \n",
    "    def __init__(self, username: str, password: str):\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        })\n",
    "        self.logged_in = False\n",
    "        self.s3 = boto3.client('s3') \n",
    "    \n",
    "    def login(self) -> bool:\n",
    "        \"\"\"Log in to CoralNet using requests session\"\"\"\n",
    "        success = False\n",
    "        try:\n",
    "            # Get login page to extract CSRF token\n",
    "            response = self.session.get(self.LOGIN_URL, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            csrf_token = soup.find(\"input\", attrs={\"name\": \"csrfmiddlewaretoken\"})\n",
    "            \n",
    "            if not csrf_token:\n",
    "                raise Exception(\"Could not find CSRF token\")\n",
    "            \n",
    "            # Prepare login data\n",
    "            data = {\n",
    "                \"username\": self.username,\n",
    "                \"password\": self.password,\n",
    "                \"csrfmiddlewaretoken\": csrf_token[\"value\"],\n",
    "            }\n",
    "            \n",
    "            headers = {\"Referer\": self.LOGIN_URL}\n",
    "            \n",
    "            # Submit login\n",
    "            login_response = self.session.post(\n",
    "                self.LOGIN_URL, \n",
    "                data=data, \n",
    "                headers=headers,\n",
    "                timeout=30,\n",
    "                allow_redirects=True\n",
    "            )\n",
    "            \n",
    "            # Check if login was successful by looking for sign out button or redirect\n",
    "            if \"Sign out\" in login_response.text or login_response.url != self.LOGIN_URL:\n",
    "                success = True\n",
    "                self.logged_in = True\n",
    "                print(\"✓ Login successful\")\n",
    "            else:\n",
    "                raise Exception(\"Login failed - invalid credentials or other error\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Could not login with {self.username}: {str(e)}\")\n",
    "        \n",
    "        return success\n",
    "    \n",
    "    def check_permissions(self, source_id: int) -> bool:\n",
    "        \"\"\"Check permissions for accessing a source\"\"\"\n",
    "        try:\n",
    "            url = f\"{self.CORALNET_URL}/source/{source_id}/\"\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            if \"Page could not be found\" in response.text:\n",
    "                raise Exception(\"Source does not exist\")\n",
    "            elif \"don't have permission\" in response.text:\n",
    "                raise Exception(\"Permission denied\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Permission check failed for source {source_id}: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def download_metadata(self, source_id: int, bucket_name: str, s3_prefix: str = \"coralnet-public-images\") -> Tuple[bool, int]:\n",
    "        \"\"\"Download metadata for a source\"\"\"\n",
    "        success = False\n",
    "        total_images_number = 0\n",
    "        \n",
    "        try:\n",
    "            url = f\"{self.CORALNET_URL}/source/{source_id}/\"\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Try to get total images count\n",
    "            try:\n",
    "                image_status_header = soup.find(\"h4\", string=\"Image Status\")\n",
    "                if image_status_header:\n",
    "                    table = image_status_header.find_next_sibling(\"table\", class_=\"detail_box_table\")\n",
    "                    if table:\n",
    "                        # Find the row where one of the <td> contains 'Total images:'\n",
    "                        total_images_row = None\n",
    "                        for tr in table.find_all(\"tr\"):\n",
    "                            tds = tr.find_all(\"td\")\n",
    "                            if any(\"Total images:\" in td.get_text() for td in tds):\n",
    "                                total_images_row = tr\n",
    "                                break\n",
    "                        if total_images_row:\n",
    "                            link = total_images_row.find(\"a\")\n",
    "                            if link:\n",
    "                                try:\n",
    "                                    total_images_number = int(link.get_text().strip().replace(\",\", \"\"))\n",
    "                                except Exception:\n",
    "                                    total_images_number = 0\n",
    "                                print(f\"Total images: {total_images_number}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Can't get number of images: {e}\")\n",
    "                total_images_number = 0\n",
    "            \n",
    "            # Extract classifier plot data from JavaScript\n",
    "            script_tags = soup.find_all(\"script\")\n",
    "            classifier_data = None\n",
    "            \n",
    "            for script in script_tags:\n",
    "                if script.string and \"Classifier overview\" in script.string:\n",
    "                    script_text = script.string\n",
    "                    start_marker = \"let classifierPlotData = \"\n",
    "                    start_index = script_text.find(start_marker)\n",
    "                    \n",
    "                    if start_index != -1:\n",
    "                        start_index += len(start_marker)\n",
    "                        end_index = script_text.find(\"];\", start_index) + 1\n",
    "                        classifier_plot_data_str = script_text[start_index:end_index]\n",
    "                        \n",
    "                        # Clean up JavaScript object notation to valid JSON\n",
    "                        classifier_plot_data_str = classifier_plot_data_str.replace(\"'\", '\"')\n",
    "                        \n",
    "                        try:\n",
    "                            classifier_data = json.loads(classifier_plot_data_str)\n",
    "                            break\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Warning: Could not parse classifier data: {e}\")\n",
    "            \n",
    "            if not classifier_data:\n",
    "                print(\"No metadata found for this source\")\n",
    "                return True, total_images_number\n",
    "            \n",
    "            # Process classifier data\n",
    "            meta = []\n",
    "            for point in classifier_data:\n",
    "                meta.append([\n",
    "                    point.get(\"x\"),        # classifier_nbr\n",
    "                    point.get(\"y\"),        # score\n",
    "                    point.get(\"nimages\"),  # nimages\n",
    "                    point.get(\"traintime\"), # traintime\n",
    "                    point.get(\"date\"),     # date\n",
    "                    point.get(\"pk\")        # src_id\n",
    "                ])\n",
    "            \n",
    "            # Save metadata\n",
    "            meta_df = pd.DataFrame(meta, columns=[\n",
    "                'Classifier nbr', 'Accuracy', 'Trained on',\n",
    "                'Date', 'Traintime', 'Global id'\n",
    "            ])\n",
    "            print(meta_df)\n",
    "\n",
    "            # Save to S3 instead of local file\n",
    "            csv_buffer = io.StringIO()\n",
    "            meta_df.to_csv(csv_buffer, index=False)\n",
    "            \n",
    "            s3_key = f\"{s3_prefix}/s{source_id}/metadata.csv\"\n",
    "\n",
    "            # Upload to S3\n",
    "            self.s3.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=s3_key,\n",
    "                Body=csv_buffer.getvalue(),\n",
    "                ContentType='text/csv'\n",
    "            )\n",
    "            \n",
    "            print(f\"✓ Metadata saved to s3://{bucket_name}/{s3_key}\")\n",
    "            success = True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Issue downloading metadata: {str(e)}\")\n",
    "        \n",
    "        return success, total_images_number\n",
    "    \n",
    "    def download_labelset(self, source_id: int, bucket_name: str, s3_prefix: str = \"coralnet-public-images\") -> bool:\n",
    "        \"\"\"Download labelset for a source\"\"\"\n",
    "        success = False\n",
    "        \n",
    "        try:\n",
    "            url = f\"{self.CORALNET_URL}/source/{source_id}/labelset/\"\n",
    "            response = self.session.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            table = soup.find('table', {'id': 'label-table'})\n",
    "            \n",
    "            if table is None:\n",
    "                raise Exception(\"Unable to find the label table\")\n",
    "            \n",
    "            rows = table.find_all('tr')\n",
    "            if not rows or len(rows) <= 1:  # Only header row or no rows\n",
    "                print(\"No labelset found for this source\")\n",
    "                return True\n",
    "            \n",
    "            label_ids = []\n",
    "            names = []\n",
    "            short_codes = []\n",
    "            \n",
    "            for row in rows[1:]:  # Skip header row\n",
    "                cells = row.find_all('td')\n",
    "                if cells:\n",
    "                    # Get label ID from link\n",
    "                    link = cells[0].find('a')\n",
    "                    if link and link.get('href'):\n",
    "                        label_id = link['href'].split('/')[-2]\n",
    "                        label_ids.append(label_id)\n",
    "                        \n",
    "                        # Get name\n",
    "                        names.append(link.get_text().strip())\n",
    "                        \n",
    "                        # Get short code (second column)\n",
    "                        if len(cells) > 1:\n",
    "                            short_codes.append(cells[1].get_text().strip())\n",
    "                        else:\n",
    "                            short_codes.append(\"\")\n",
    "            \n",
    "            if label_ids:\n",
    "                labelset_df = pd.DataFrame({\n",
    "                    'Label ID': label_ids,\n",
    "                    'Name': names,\n",
    "                    'Short Code': short_codes\n",
    "                })\n",
    "                \n",
    "                # Save to S3 instead of local file\n",
    "                csv_buffer = io.StringIO()\n",
    "                labelset_df.to_csv(csv_buffer, index=False)\n",
    "                \n",
    "                s3_key = f\"{s3_prefix}/s{source_id}/labelset.csv\"\n",
    "\n",
    "                # Upload to S3\n",
    "                self.s3.put_object(\n",
    "                    Bucket=bucket_name,\n",
    "                    Key=s3_key,\n",
    "                    Body=csv_buffer.getvalue(),\n",
    "                    ContentType='text/csv'\n",
    "                )\n",
    "                # filepath = os.path.join(output_dir, \"labelset.csv\")\n",
    "                # labelset_df.to_csv(filepath, index=False)\n",
    "                \n",
    "                print(f\"✓ Labelset saved to s3://{bucket_name}/{s3_key}\")\n",
    "                success = True\n",
    "                    \n",
    "            else:\n",
    "                print(\"No labels found in labelset\")\n",
    "                success = True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Issue downloading labelset: {str(e)}\")\n",
    "        \n",
    "        return success\n",
    "    \n",
    "    def download_annotations(self, source_id: int, bucket_name: str, s3_prefix: str = \"coralnet-public-images\") -> bool:\n",
    "        \"\"\"Download annotations for a source\"\"\"\n",
    "        success = False\n",
    "        \n",
    "        try:\n",
    "            # First, get the browse images page to extract form data\n",
    "            browse_url = f\"{self.CORALNET_URL}/source/{source_id}/browse/images/\"\n",
    "            response = self.session.get(browse_url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find the export form\n",
    "            export_form = soup.find('form', {'id': 'export-annotations-prep-form'})\n",
    "            if not export_form:\n",
    "                raise Exception(\"Could not find export annotations form\")\n",
    "            \n",
    "            # Extract CSRF token\n",
    "            csrf_token = export_form.find('input', {'name': 'csrfmiddlewaretoken'})\n",
    "            if not csrf_token:\n",
    "                raise Exception(\"Could not find CSRF token in export form\")\n",
    "            \n",
    "            # Prepare form data for annotation export\n",
    "            form_data = {\n",
    "                'csrfmiddlewaretoken': csrf_token['value'],\n",
    "                'browse_action': 'export_annotations',\n",
    "                'image_select_type': 'all',\n",
    "                'label_format': 'both',\n",
    "                # Add all optional columns\n",
    "                'optional_columns': [\n",
    "                    'annotator_info',\n",
    "                    \"metadata_date_aux\",\n",
    "                    \"metadata_other\",\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            export_request_url = f\"{self.CORALNET_URL}/source/{source_id}/annotation/export_prep/\"\n",
    "            # Submit the export request\n",
    "            export_response = self.session.post(\n",
    "                export_request_url,\n",
    "                headers={'Referer': browse_url},\n",
    "                data=form_data,\n",
    "                timeout=120,  # Longer timeout for processing\n",
    "                allow_redirects=True\n",
    "            )\n",
    "            \n",
    "            export_timestamp = export_response.json()['session_data_timestamp']\n",
    "            download_annotations_url = f\"https://coralnet.ucsd.edu/source/{source_id}/export/serve/?session_data_timestamp={export_timestamp}\"\n",
    "            download_annotations_response = self.session.get(download_annotations_url, timeout=60)\n",
    "            download_annotations_response.raise_for_status()\n",
    "\n",
    "            df_annotations = pd.read_csv(io.StringIO(download_annotations_response.text))\n",
    "            # Save to S3 instead of local file\n",
    "            csv_buffer = io.StringIO()\n",
    "            df_annotations.to_csv(csv_buffer, index=False)\n",
    "            \n",
    "            s3_key = f\"{s3_prefix}/s{source_id}/annotations.csv\"\n",
    "\n",
    "            # Upload to S3\n",
    "            self.s3.put_object(\n",
    "                Bucket=bucket_name,\n",
    "                Key=s3_key,\n",
    "                Body=csv_buffer.getvalue(),\n",
    "                ContentType='text/csv'\n",
    "            )\n",
    "            # filepath = os.path.join(output_dir, \"labelset.csv\")\n",
    "            # labelset_df.to_csv(filepath, index=False)\n",
    "            \n",
    "            print(f\"✓ Annotations saved to s3://{bucket_name}/{s3_key}\")\n",
    "            success = True\n",
    "            # annotations_file = os.path.join(output_dir, \"annotations.csv\")\n",
    "            # if not os.path.exists(output_dir):\n",
    "            #     os.makedirs(output_dir)\n",
    "            # df_annotations.to_csv(annotations_file, index=False)\n",
    "            # if os.path.exists(annotations_file) and os.path.getsize(annotations_file) > 0:\n",
    "            #     print(f\"✓ Annotations saved to {annotations_file}\")\n",
    "            #     success = True\n",
    "            # else:\n",
    "            #     raise Exception(\"Downloaded annotations file is empty\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Issue downloading annotations: {str(e)}\")\n",
    "            success = False  # Don't fail the entire process\n",
    "        \n",
    "        return success\n",
    "    \n",
    "    def get_images_on_page(self, browse_url: str) -> tuple[dict[str, str], Optional[str]]:\n",
    "        \"\"\"\n",
    "        Get a dictionary of image names and their URLs from the CoralNet browse page\n",
    "        \n",
    "        Args:\n",
    "            session: requests.Session object with valid CoralNet login\n",
    "            browse_url: URL of the browse images page\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary with image names as keys and their URLs as values\n",
    "        \"\"\"\n",
    "        images = {}\n",
    "        \n",
    "        response = self.session.get(browse_url, timeout=30)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        thumb_wrappers = soup.find_all('span', class_='thumb_wrapper')\n",
    "        for wrapper in thumb_wrappers:\n",
    "            link = wrapper.find('a')\n",
    "            img = wrapper.find('img')\n",
    "            if link and img:\n",
    "                image_name = img.get('title', '')\n",
    "                image_url = link.get('href', '')\n",
    "                \n",
    "                if image_name and image_url:\n",
    "                    images[image_name] = image_url\n",
    "\n",
    "        next_page_element = soup.find('a', title='Next page')\n",
    "        next_page_url = next_page_element.get('href') if next_page_element else None\n",
    "                    \n",
    "        return images, next_page_url\n",
    "\n",
    "    def get_images(self, source_id: int) -> Tuple[Optional[pd.DataFrame], bool]:\n",
    "        \"\"\"\n",
    "        Get a DataFrame of all images from a CoralNet source\n",
    "        \n",
    "        Args:\n",
    "            session: requests.Session object with valid CoralNet login\n",
    "            source_id: ID of the CoralNet source\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing image names and URLs\n",
    "        \"\"\"\n",
    "        images = None\n",
    "        success = False\n",
    "\n",
    "        base_url = f'{self.CORALNET_URL}/source/{source_id}/browse/images'\n",
    "        all_images = {}\n",
    "        try: \n",
    "            imgs, next_page = self.get_images_on_page(base_url)\n",
    "            all_images.update(imgs)\n",
    "            p_bar = tqdm(desc=\"Fetching images\", unit=\"page\")\n",
    "            while next_page:\n",
    "                imgs, next_page = self.get_images_on_page(f\"{base_url}/{next_page}\")\n",
    "                all_images.update(imgs)\n",
    "                p_bar.update(1)\n",
    "            p_bar.close()\n",
    "            success = True\n",
    "            images = pd.DataFrame(list(all_images.items()), columns=['Name', 'Image Page'])\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Issue retrieving images: {str(e)}\")\n",
    "        return images, success\n",
    "\n",
    "    def get_image_urls(self, image_page_urls: List[str]) -> List[Optional[str]]:\n",
    "        \"\"\"\n",
    "        Get the direct image URLs from the CoralNet image page URLs\n",
    "\n",
    "        Args:\n",
    "            image_page_url: URL of the CoralNet image page\n",
    "\n",
    "        Returns:\n",
    "            str or None: Direct image URL or None if not found\n",
    "        \"\"\"\n",
    "        image_urls = []\n",
    "        for image_page_url in tqdm(image_page_urls, desc=\"Fetching image URLs\", unit=\"image\"):\n",
    "            image_page_url = f'https://coralnet.ucsd.edu{image_page_url}'\n",
    "            image_view_response = urllib.request.urlopen(image_page_url)\n",
    "            response_soup = BeautifulSoup(\n",
    "                image_view_response.read(), 'html.parser')\n",
    "\n",
    "            original_img_elements = response_soup.select(\n",
    "                'div#original_image_container > img')\n",
    "            if not original_img_elements:\n",
    "                raise ValueError(\n",
    "                    f\"CoralNet image {image_page_url}: couldn't find image on the\"\n",
    "                    f\" image-view page. Maybe it's in a private source.\")\n",
    "            image_url = original_img_elements[0].attrs.get('src')\n",
    "            image_urls.append(image_url)\n",
    "\n",
    "        return image_urls\n",
    "    \n",
    "    @staticmethod\n",
    "    def download_image(url: str, path: str, timeout: int = 30) -> Tuple[str, bool]:\n",
    "        \"\"\"Download a single image\"\"\"\n",
    "        if os.path.exists(path):\n",
    "            return path, True\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=timeout, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "            \n",
    "            with open(path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            \n",
    "            if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "                return path, True\n",
    "            else:\n",
    "                return path, False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to download {url}: {e}\")\n",
    "            return path, False\n",
    "    \n",
    "    def download_image_to_s3(self, url: str, bucket_name: str, s3_key: str, timeout: int = 30) -> Tuple[str, bool]:\n",
    "        \"\"\"Download a single image and upload directly to S3\"\"\"\n",
    "        try:\n",
    "            # Check if object already exists in S3\n",
    "            try:\n",
    "                self.s3.head_object(Bucket=bucket_name, Key=s3_key)\n",
    "                return s3_key, True  # File already exists\n",
    "            except self.s3.exceptions.ClientError:\n",
    "                pass  # File doesn't exist, continue with download\n",
    "            \n",
    "            response = requests.get(url, timeout=timeout, stream=True)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Upload directly to S3 without saving locally\n",
    "            self.s3.upload_fileobj(\n",
    "                response.raw,\n",
    "                bucket_name,\n",
    "                s3_key,\n",
    "                ExtraArgs={'ContentType': 'image/jpeg'}\n",
    "            )\n",
    "            \n",
    "            return s3_key, True\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to download {url} to S3: {e}\")\n",
    "            return s3_key, False\n",
    "            \n",
    "    def download_images(self, images_df: pd.DataFrame, source_id: int, bucket_name: str, s3_prefix: str = \"coralnet-public-images\") -> None:\n",
    "        \"\"\"Download all images from a DataFrame\"\"\"\n",
    "        # Save image list\n",
    "        csv_buffer = io.StringIO()\n",
    "        images_df.to_csv(csv_buffer, index=False)\n",
    "        \n",
    "        s3_key = f\"{s3_prefix}/s{source_id}/image_list.csv\"\n",
    "\n",
    "        # Upload to S3\n",
    "        self.s3.put_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=s3_key,\n",
    "            Body=csv_buffer.getvalue(),\n",
    "            ContentType='text/csv'\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Images saved to s3://{bucket_name}/{s3_key}\")\n",
    "        success = True\n",
    "        \n",
    "        # Filter out rows without URLs\n",
    "        valid_images = images_df[images_df['Image URL'].notna()]\n",
    "        \n",
    "        if valid_images.empty:\n",
    "            print(\"Warning: No valid image URLs found\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Downloading {len(valid_images)} images...\")\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=min(8, os.cpu_count() or 4)) as executor:\n",
    "            futures = []\n",
    "            for _, row in valid_images.iterrows():\n",
    "                name = row[\"Image Page\"].replace(\"/image/\", \"\").replace(\"/view/\", \"\")#.split(\"image/\")[1].split(\"/view/\")[0] #row['Name']\n",
    "                url = row['Image URL']\n",
    "                clean_name = name + \".jpg\" # name.replace(\".jpg\", \"\").replace(\" - Confirmed\", \"\") + \".jpg\"\n",
    "                s3_key = f\"{s3_prefix}/s{source_id}/images/{clean_name}\"\n",
    "\n",
    "                # path = os.path.join(image_dir.replace(\".jpg\", \"\"), name.replace(\".jpg\", \"\").replace(\" - Confirmed\", \"\") + \".jpg\")\n",
    "                # print(path)\n",
    "                futures.append(executor.submit(self.download_image_to_s3, url, bucket_name, s3_key))\n",
    "\n",
    "                # futures.append(executor.submit(self.download_image, url, path))\n",
    "            \n",
    "            completed = 0\n",
    "            successful = 0\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                try:\n",
    "                    path, success = future.result()\n",
    "                    if success:\n",
    "                        successful += 1\n",
    "                    else:\n",
    "                        print(f\"Warning: Failed to download {os.path.basename(path)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: {str(e)}\")\n",
    "                \n",
    "                completed += 1\n",
    "                if completed % 10 == 0 or completed == len(futures):\n",
    "                    print(f\"Progress: {completed}/{len(futures)} images processed\")\n",
    "        \n",
    "        print(f\"✓ Uploaded {successful}/{len(valid_images)} images to s3://{bucket_name}/{s3_prefix}/s{source_id}/images/\")\n",
    "\n",
    "    def download_source(self, source_id: int, bucket_name: str, \n",
    "                        s3_prefix: str = \"coralnet-public-images\",\n",
    "                        download_metadata: bool = True,\n",
    "                        download_labelset: bool = True,\n",
    "                        download_annotations: bool = True,\n",
    "                        download_images: bool = True) -> bool:\n",
    "        \"\"\"Download all data for a source\"\"\"\n",
    "        print(f\"\\n=== Downloading Source {source_id} ===\")\n",
    "        \n",
    "        \n",
    "        # Login if needed\n",
    "        if not self.logged_in:\n",
    "            if not self.login():\n",
    "                raise Exception(\"Failed to login to CoralNet\")\n",
    "        \n",
    "        # Check permissions\n",
    "        if not self.check_permissions(source_id):\n",
    "            raise Exception(f\"Cannot access source {source_id}\")\n",
    "        \n",
    "        success = True\n",
    "        n_images = 0\n",
    "        \n",
    "        # Download metadata\n",
    "        if download_metadata:\n",
    "            metadata_success, n_images = self.download_metadata(source_id, bucket_name=bucket_name, s3_prefix=s3_prefix)\n",
    "            if not metadata_success:\n",
    "                print(\"Warning: Failed to download metadata\")\n",
    "            \n",
    "            if n_images == 0:\n",
    "                print(\"Source appears to be empty\")\n",
    "                # os.makedirs(os.path.join(source_dir, \"empty\"), exist_ok=True)\n",
    "                return True\n",
    "        \n",
    "        # Download labelset\n",
    "        if download_labelset:\n",
    "            if not self.download_labelset(source_id, bucket_name=bucket_name, s3_prefix=s3_prefix):\n",
    "                print(\"Warning: Failed to download labelset\")\n",
    "        \n",
    "        # Download annotations\n",
    "        if download_annotations:\n",
    "            if not self.download_annotations(source_id, bucket_name=bucket_name, s3_prefix=s3_prefix):\n",
    "                print(\"Warning: Failed to download annotations\")\n",
    "        \n",
    "        # Download images\n",
    "        if download_images:\n",
    "            images_df, images_success = self.get_images(source_id)\n",
    "            if images_success and images_df is not None and len(images_df) > 0:\n",
    "                # Get image URLs\n",
    "                image_urls = self.get_image_urls(images_df['Image Page'].tolist())\n",
    "                images_df['Image URL'] = image_urls\n",
    "                \n",
    "                # Download images\n",
    "                self.download_images(images_df, source_id, bucket_name=bucket_name, s3_prefix=s3_prefix)\n",
    "            else:\n",
    "                print(\"Warning: No images found or failed to retrieve image list\")\n",
    "        \n",
    "        print(f\"✓ Completed downloading source {source_id}\")\n",
    "        return success\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        if self.session:\n",
    "            self.session.close()\n",
    "        self.logged_in = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d8f49f",
   "metadata": {},
   "source": [
    "# Run on S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73c79b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ids = [\"3329\", \"3341\", \"23\", \"1073\"]\n",
    "\n",
    "bucket_name = \"dev-datamermaid-sm-sources\"\n",
    "prefix = \"coralnet-public-images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aa71ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = input(\"CoralNet username: \")\n",
    "password = getpass.getpass(\"CoralNet password: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a32da745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Downloading Source 3329 ===\n",
      "✓ Login successful\n",
      "Total images: 20\n",
      "No metadata found for this source\n",
      "✓ Labelset saved to s3://dev-datamermaid-sm-sources/coralnet-public-images/s3329/labelset.csv\n",
      "✓ Annotations saved to s3://dev-datamermaid-sm-sources/coralnet-public-images/s3329/annotations.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d0805c109f4c46a850bef1bec52d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching images: 0page [00:00, ?page/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5e747070c54db29fbc3e0230d3236e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching image URLs:   0%|          | 0/20 [00:00<?, ?image/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Images saved to s3://dev-datamermaid-sm-sources/coralnet-public-images/s3329/image_list.csv\n",
      "Downloading 20 images...\n",
      "Progress: 10/20 images processed\n",
      "Progress: 20/20 images processed\n",
      "✓ Uploaded 20/20 images to s3://dev-datamermaid-sm-sources/coralnet-public-images/s3329/images/\n",
      "✓ Completed downloading source 3329\n",
      "\n",
      "=== Downloading Source 3341 ===\n",
      "No metadata found for this source\n",
      "Source appears to be empty\n",
      "\n",
      "=== Downloading Source 23 ===\n",
      "Total images: 750\n",
      "   Classifier nbr  Accuracy  Trained on     Date      Traintime Global id\n",
      "0               1        67          20  0:00:06  Nov. 21, 2016        21\n",
      "1               2        76         408  0:02:32  Nov. 27, 2016       739\n",
      "2               3        78         750  0:09:24   Dec. 5, 2016      1356\n",
      "✓ Metadata saved to s3://dev-datamermaid-sm-sources/coralnet-public-images/s23/metadata.csv\n",
      "✓ Labelset saved to s3://dev-datamermaid-sm-sources/coralnet-public-images/s23/labelset.csv\n",
      "✓ Annotations saved to s3://dev-datamermaid-sm-sources/coralnet-public-images/s23/annotations.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff0a5798d894649bfd054ebb0ba86a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching images: 0page [00:00, ?page/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a206766d2fe46fda144dbdf73401a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching image URLs:   0%|          | 0/750 [00:00<?, ?image/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Images saved to s3://dev-datamermaid-sm-sources/coralnet-public-images/s23/image_list.csv\n",
      "Downloading 750 images...\n",
      "Progress: 10/750 images processed\n",
      "Progress: 20/750 images processed\n",
      "Progress: 30/750 images processed\n",
      "Progress: 40/750 images processed\n",
      "Progress: 50/750 images processed\n",
      "Progress: 60/750 images processed\n",
      "Progress: 70/750 images processed\n",
      "Progress: 80/750 images processed\n",
      "Progress: 90/750 images processed\n",
      "Progress: 100/750 images processed\n",
      "Progress: 110/750 images processed\n",
      "Progress: 120/750 images processed\n",
      "Progress: 130/750 images processed\n",
      "Progress: 140/750 images processed\n",
      "Progress: 150/750 images processed\n",
      "Progress: 160/750 images processed\n",
      "Progress: 170/750 images processed\n",
      "Progress: 180/750 images processed\n",
      "Progress: 190/750 images processed\n",
      "Progress: 200/750 images processed\n",
      "Progress: 210/750 images processed\n",
      "Progress: 220/750 images processed\n",
      "Progress: 230/750 images processed\n",
      "Progress: 240/750 images processed\n",
      "Progress: 250/750 images processed\n",
      "Progress: 260/750 images processed\n",
      "Progress: 270/750 images processed\n",
      "Progress: 280/750 images processed\n",
      "Progress: 290/750 images processed\n",
      "Progress: 300/750 images processed\n",
      "Progress: 310/750 images processed\n",
      "Progress: 320/750 images processed\n",
      "Progress: 330/750 images processed\n",
      "Progress: 340/750 images processed\n",
      "Progress: 350/750 images processed\n",
      "Progress: 360/750 images processed\n",
      "Progress: 370/750 images processed\n",
      "Progress: 380/750 images processed\n",
      "Progress: 390/750 images processed\n",
      "Progress: 400/750 images processed\n",
      "Progress: 410/750 images processed\n",
      "Progress: 420/750 images processed\n",
      "Progress: 430/750 images processed\n",
      "Progress: 440/750 images processed\n",
      "Progress: 450/750 images processed\n",
      "Progress: 460/750 images processed\n",
      "Progress: 470/750 images processed\n",
      "Progress: 480/750 images processed\n",
      "Progress: 490/750 images processed\n",
      "Progress: 500/750 images processed\n",
      "Progress: 510/750 images processed\n",
      "Progress: 520/750 images processed\n",
      "Progress: 530/750 images processed\n",
      "Progress: 540/750 images processed\n",
      "Progress: 550/750 images processed\n",
      "Progress: 560/750 images processed\n",
      "Progress: 570/750 images processed\n",
      "Progress: 580/750 images processed\n",
      "Progress: 590/750 images processed\n",
      "Progress: 600/750 images processed\n",
      "Progress: 610/750 images processed\n",
      "Progress: 620/750 images processed\n",
      "Progress: 630/750 images processed\n",
      "Progress: 640/750 images processed\n",
      "Progress: 650/750 images processed\n",
      "Progress: 660/750 images processed\n",
      "Progress: 670/750 images processed\n",
      "Progress: 680/750 images processed\n",
      "Progress: 690/750 images processed\n",
      "Progress: 700/750 images processed\n",
      "Progress: 710/750 images processed\n",
      "Progress: 720/750 images processed\n",
      "Progress: 730/750 images processed\n",
      "Progress: 740/750 images processed\n",
      "Progress: 750/750 images processed\n",
      "✓ Uploaded 750/750 images to s3://dev-datamermaid-sm-sources/coralnet-public-images/s23/images/\n",
      "✓ Completed downloading source 23\n",
      "\n",
      "=== Downloading Source 1073 ===\n",
      "Total images: 225\n",
      "   Classifier nbr  Accuracy  Trained on     Date      Traintime Global id\n",
      "0               1        52          20  0:00:52  June 15, 2018      5757\n",
      "1               2        63          23  0:01:00  June 15, 2018      5759\n",
      "2               3        57          29  0:01:13  June 16, 2018      5766\n",
      "3               4        51          32  0:01:27  June 16, 2018      5767\n",
      "4               5        57          36  0:01:40  June 16, 2018      5768\n",
      "5               6        53          45  0:02:09  June 16, 2018      5770\n",
      "6               7        56          50  0:02:14  June 16, 2018      5771\n",
      "7               8        57          56  0:02:42  June 16, 2018      5772\n",
      "8               9        65         114  0:05:55  June 18, 2018      5800\n",
      "✓ Metadata saved to s3://dev-datamermaid-sm-sources/coralnet-public-images/s1073/metadata.csv\n",
      "✓ Labelset saved to s3://dev-datamermaid-sm-sources/coralnet-public-images/s1073/labelset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1123/4248604748.py:314: DtypeWarning: Columns (10,11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_annotations = pd.read_csv(io.StringIO(download_annotations_response.text))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Annotations saved to s3://dev-datamermaid-sm-sources/coralnet-public-images/s1073/annotations.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1901378b53841b183d05bb7270cdfbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching images: 0page [00:00, ?page/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4180f3bf60448df806bcab97b78524b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching image URLs:   0%|          | 0/225 [00:00<?, ?image/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Images saved to s3://dev-datamermaid-sm-sources/coralnet-public-images/s1073/image_list.csv\n",
      "Downloading 225 images...\n",
      "Progress: 10/225 images processed\n",
      "Progress: 20/225 images processed\n",
      "Progress: 30/225 images processed\n",
      "Progress: 40/225 images processed\n",
      "Progress: 50/225 images processed\n",
      "Progress: 60/225 images processed\n",
      "Progress: 70/225 images processed\n",
      "Progress: 80/225 images processed\n",
      "Progress: 90/225 images processed\n",
      "Progress: 100/225 images processed\n",
      "Progress: 110/225 images processed\n",
      "Progress: 120/225 images processed\n",
      "Progress: 130/225 images processed\n",
      "Progress: 140/225 images processed\n",
      "Progress: 150/225 images processed\n",
      "Progress: 160/225 images processed\n",
      "Progress: 170/225 images processed\n",
      "Progress: 180/225 images processed\n",
      "Progress: 190/225 images processed\n",
      "Progress: 200/225 images processed\n",
      "Progress: 210/225 images processed\n",
      "Progress: 220/225 images processed\n",
      "Progress: 225/225 images processed\n",
      "✓ Uploaded 225/225 images to s3://dev-datamermaid-sm-sources/coralnet-public-images/s1073/images/\n",
      "✓ Completed downloading source 1073\n"
     ]
    }
   ],
   "source": [
    "downloader = CoralNetDownloader(username=username, password=password)\n",
    "for source_id in source_ids:\n",
    "    downloader.download_source(source_id=source_id, bucket_name=bucket_name, s3_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea10ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata_success, n_images = downloader.download_metadata(source_id, bucket_name = \"dev-datamermaid-sm-sources\", s3_prefix = \"coralnet-public-images\")\n",
    "# downloader.download_labelset(source_id, bucket_name = \"dev-datamermaid-sm-sources\", s3_prefix = \"coralnet-public-images\")\n",
    "# downloader.download_annotations(source_id, bucket_name = \"dev-datamermaid-sm-sources\", s3_prefix = \"coralnet-public-images\")\n",
    "\n",
    "# images_df, images_success = downloader.get_images(source_id)\n",
    "# if images_success and images_df is not None and len(images_df) > 0:\n",
    "#     image_urls = downloader.get_image_urls(images_df['Image Page'].tolist())\n",
    "#     images_df['Image URL'] = image_urls\n",
    "#     downloader.download_images(images_df, source_id, bucket_name = \"dev-datamermaid-sm-sources\", s3_prefix = \"coralnet-public-images\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
