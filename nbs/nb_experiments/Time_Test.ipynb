{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787a9f13-cf21-4463-b634-be5709a28b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoreload extension\n",
    "%load_ext autoreload\n",
    "\n",
    "# Set autoreload mode\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9723b063-e597-46e1-8a5b-c91d1df9ba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import mermaidseg.datasets.dataset\n",
    "import numpy as np\n",
    "from mermaidseg.io import setup_config, get_parser, update_config_with_args\n",
    "import copy\n",
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2718d11-559e-48ca-965b-52b5accf8542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device 0: Tesla T4\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "for i in range(device_count):\n",
    "    print(f\"CUDA Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51bae33c-af67-41d4-8512-b9e100af7a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 15:00:43.933724: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-17 15:00:43.956509: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765983643.986520  110916 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765983643.995859  110916 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-12-17 15:00:44.024129: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from mermaidseg.model.meta import MetaModel\n",
    "from mermaidseg.model.eval import EvaluatorSemanticSegmentation\n",
    "from mermaidseg.logger import Logger\n",
    "from mermaidseg.model.train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f319c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start off with a configuration file\n",
    "cfg = setup_config(config_path='../configs/linear-dinov3-concept.yaml', config_base_path='../configs/concept_mermaid.yaml')\n",
    "\n",
    "# Update the initial configuration file with command line arguments \n",
    "# (in the case of a notebook run these can be defined explicitly here)\n",
    "args_input = \"--run-name=dinov3-test-concept-run --batch-size=2 --epochs=5 --log-epochs=1\"\n",
    "args_input = args_input.split(\" \")\n",
    "\n",
    "parser = get_parser()\n",
    "args = parser.parse_args(args_input)\n",
    "\n",
    "cfg = update_config_with_args(cfg, args)\n",
    "cfg_logger = copy.deepcopy(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cf45be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = {}\n",
    "for split in cfg.augmentation:\n",
    "    transforms[split] = A.Compose(\n",
    "        [\n",
    "            getattr(A, transform_name)(**transform_params) for transform_name, transform_params\n",
    "                                                                 in cfg.augmentation[split].items()\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e065e488",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = cfg.data.pop(\"name\", None)\n",
    "batch_size = cfg.data.pop(\"batch_size\", 4)\n",
    "whitelist_sources = cfg.data.pop(\"whitelist_sources\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46370320",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {}\n",
    "dataset_dict[\"train\"] = getattr(mermaidseg.datasets.dataset, dataset_name)(transform = transforms[split], **cfg.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78dad8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8073"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_dict[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "016d5658",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(dataset_dict[\"train\"])\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.15 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset_dict[\"train\"], [train_size, val_size, test_size], generator=generator)\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, range(5000))\n",
    "val_dataset = torch.utils.data.Subset(val_dataset, range(1000))\n",
    "test_dataset = torch.utils.data.Subset(test_dataset, range(1000))\n",
    "# train_dataset = torch.utils.data.Subset(dataset_dict[\"train\"], range(3000))\n",
    "# val_dataset = torch.utils.data.Subset(dataset_dict[\"val\"], range(500))\n",
    "# test_dataset = torch.utils.data.Subset(dataset_dict[\"test\"], range(500))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1, drop_last=True, collate_fn = dataset_dict[\"train\"].collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1, drop_last=True, collate_fn = dataset_dict[\"train\"].collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1, drop_last=True, collate_fn = dataset_dict[\"train\"].collate_fn)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True, collate_fn = dataset_dict[\"train\"].collate_fn)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True, collate_fn = dataset_dict[\"val\"].collate_fn)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True, collate_fn = dataset_dict[\"test\"].collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9300c992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 2500\n",
      "Number of validation batches: 500\n",
      "Number of test batches: 500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n",
    "print(f\"Number of test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f10f0953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict[\"train\"].num_classes, dataset_dict[\"train\"].num_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7632f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = MetaModel(run_name = cfg.run_name, \n",
    "                       num_classes = dataset_dict[\"train\"].num_classes,\n",
    "                       num_concepts = dataset_dict[\"train\"].num_concepts,\n",
    "                       device = device,\n",
    "                       model_kwargs = cfg.model,\n",
    "                       training_mode = cfg.training_mode,\n",
    "                       training_kwargs = cfg.training,\n",
    "                       concept_matrix = dataset_dict[\"train\"].benthic_concept_matrix,\n",
    "                       conceptid2labelid = dataset_dict[\"train\"].conceptid2labelid,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8bab30",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "359d8165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "723303ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b94909",
   "metadata": {},
   "source": [
    "## Typical Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d579c9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [04:33<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "time_dict[\"typical\"] = {\"data_io\":0, \"forward\":0}\n",
    "num_iterations = 100\n",
    "for i in tqdm.tqdm(range(num_iterations)):\n",
    "        start_time = time.time()\n",
    "        data = next(iter(train_loader))\n",
    "        time_dict[\"typical\"][\"data_io\"] += (time.time() - start_time)/num_iterations\n",
    "\n",
    "        _, labels = data\n",
    "        labels = labels.long().to(meta_model.device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss, outputs, concept_outputs = meta_model.batch_predict_loss(data)\n",
    "        time_dict[\"typical\"][\"forward\"] += (time.time() - start_time)/num_iterations\n",
    "        \n",
    "        del loss, outputs, concept_outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acbbbfcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_io': 1.9751221752166752, 'forward': 0.691829288005829}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_dict[\"typical\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f7746",
   "metadata": {},
   "source": [
    "## Num Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49b48464",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:20<00:00,  2.81s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True, collate_fn = dataset_dict[\"train\"].collate_fn)\n",
    "time_dict[\"num_workers=2\"] = {\"data_io\":0, \"forward\":0}\n",
    "num_iterations = 50\n",
    "for i in tqdm.tqdm(range(num_iterations)):\n",
    "        start_time = time.time()\n",
    "        data = next(iter(train_loader))\n",
    "        time_dict[\"num_workers=2\"][\"data_io\"] += (time.time() - start_time)/num_iterations\n",
    "\n",
    "        _, labels = data\n",
    "        labels = labels.long().to(meta_model.device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss, outputs, concept_outputs = meta_model.batch_predict_loss(data)\n",
    "        time_dict[\"num_workers=2\"][\"forward\"] += (time.time() - start_time)/num_iterations\n",
    "        \n",
    "        del loss, outputs, concept_outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20cf31cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_io': 2.024768481254578, 'forward': 0.7219918203353882}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_dict[\"num_workers=2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1524e93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:35<00:00,  3.11s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True, collate_fn = dataset_dict[\"train\"].collate_fn)\n",
    "time_dict[\"num_workers=4\"] = {\"data_io\":0, \"forward\":0}\n",
    "num_iterations = 50\n",
    "for i in tqdm.tqdm(range(num_iterations)):\n",
    "        start_time = time.time()\n",
    "        data = next(iter(train_loader))\n",
    "        time_dict[\"num_workers=4\"][\"data_io\"] += (time.time() - start_time)/num_iterations\n",
    "\n",
    "        _, labels = data\n",
    "        labels = labels.long().to(meta_model.device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss, outputs, concept_outputs = meta_model.batch_predict_loss(data)\n",
    "        time_dict[\"num_workers=4\"][\"forward\"] += (time.time() - start_time)/num_iterations\n",
    "        \n",
    "        del loss, outputs, concept_outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c48c93d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_io': 2.2996981573104858, 'forward': 0.7402305412292479}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_dict[\"num_workers=4\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf1135",
   "metadata": {},
   "source": [
    "## Local Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0ea2742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "region_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "region_name",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "5a487096-3693-431c-8bf5-2cbfda2f127e",
       "rows": [
        [
         "0",
         "00059a47-03b8-47f3-adf6-3ab5616922cf",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "1",
         "00086e76-2b0d-48ff-a25b-31020c226047",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "2",
         "00138e67-611c-4e04-a382-46e0484f2f95",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "3",
         "0015772a-fcc3-4bd8-bfb8-2a3b67520f35",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "4",
         "001580fa-3324-4053-a74b-c5ef08d49d07",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "5",
         "001f365c-e6f9-4ce2-a68b-6fe493ed80a6",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "6",
         "001fabba-820e-4ae4-821e-0150cc013f41",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "7",
         "0032dba6-8357-42e2-bace-988f99032286",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "8",
         "003bdf79-ce3e-4d9f-9e5a-60313ddeb97e",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "9",
         "003e151e-b3c3-419f-9453-c968c88d4870",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "10",
         "004815d3-7a32-4264-a1ff-94cbee4c78b7",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "11",
         "004ed958-bdd4-453d-83ed-bf773c78bfc8",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "12",
         "0055f123-0b6a-4c30-ac51-db7345abd320",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "13",
         "0055fc63-bcce-485b-bb3b-5384c08ad87f",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "14",
         "005b08d1-9f04-40d9-b636-113316faaaca",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "15",
         "0067ed24-8daf-4ea9-a216-1ff5233268ac",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "16",
         "006851ca-0cd1-4514-b92c-289f505f83a2",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "17",
         "0072906a-2d45-41ee-84ab-855d5f740028",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "18",
         "00751fc7-b295-4a46-8576-745a94274168",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "19",
         "0075c15f-d658-491c-927f-e340a14acfed",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "20",
         "0076c1b6-dd8a-4742-829b-e5480b60db5a",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "21",
         "007e2898-f644-42bc-926e-86a08b8c7b1a",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "22",
         "00843672-9943-46ae-b77a-acb5770f8afb",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "23",
         "0084b455-ecf1-41ac-a5b0-0fa34cbff18d",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "24",
         "008ccb28-1b6c-4f7a-ac2c-0921745d9e41",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "25",
         "00938ce1-cc82-4f8d-ae40-db8b10d63ce6",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "26",
         "009b60a4-09c5-49cf-870e-5a0c01702972",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "27",
         "009c8e37-c118-494a-83b4-d1dfa19b57bc",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "28",
         "00a8171e-32e2-48a5-91b1-74095feb4713",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "29",
         "00ab4baa-1893-40f5-9d0c-9af8482b1ddf",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "30",
         "00ad9316-b191-4c15-bee2-7e6f9313989d",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "31",
         "00ccbbe5-50ea-4978-ae8d-93d045164924",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "32",
         "00cce52b-c0cd-46f6-963c-564970909410",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "33",
         "00cee838-bf3a-4e04-8bd3-0cb0362c939e",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "34",
         "00d067f4-f93b-4728-ab52-e0ba32b83635",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "35",
         "00d6100f-1bc5-42d7-b249-b292c990a130",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "36",
         "00d6ad04-2be2-41db-bbca-93173fc2b205",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "37",
         "00da6fa1-9b9c-4a59-bbcb-d9c0271cc3b0",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "38",
         "00dabf19-1d9a-44a8-b1f9-e4495e6f008f",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "39",
         "00f9dc68-6cee-464f-a7d9-325d661ac7c3",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "40",
         "00fd1a72-0f9e-4600-9ca4-b9a0e5269588",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "41",
         "0103e2eb-16e5-4bad-b341-3ce746308cee",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "42",
         "011011fb-9702-4842-8d88-3b5b4b9deedc",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "43",
         "01121fc7-85f7-4d1c-acc1-ba9ce6a63d60",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "44",
         "01129456-5107-4b21-aa3d-a3bd0815bd60",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "45",
         "0124737d-da11-48e7-8c98-3499f49a6232",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "46",
         "013171f5-2106-4d89-9ccb-35d739f0d933",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "47",
         "01325251-1985-495f-a741-7daa90c23d4b",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ],
        [
         "48",
         "01350d84-802b-4a6a-9733-2ff9ab790bf4",
         "983267a0-7349-4d3e-a23e-fb9353ca8ba5",
         "Central Indo-Pacific"
        ],
        [
         "49",
         "013697c4-33cd-4924-912d-642731a093e3",
         "1d31d9ea-e78d-438b-8667-0d63d1aba257",
         "Western Indo-Pacific"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 8073
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>region_id</th>\n",
       "      <th>region_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00059a47-03b8-47f3-adf6-3ab5616922cf</td>\n",
       "      <td>1d31d9ea-e78d-438b-8667-0d63d1aba257</td>\n",
       "      <td>Western Indo-Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00086e76-2b0d-48ff-a25b-31020c226047</td>\n",
       "      <td>1d31d9ea-e78d-438b-8667-0d63d1aba257</td>\n",
       "      <td>Western Indo-Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00138e67-611c-4e04-a382-46e0484f2f95</td>\n",
       "      <td>1d31d9ea-e78d-438b-8667-0d63d1aba257</td>\n",
       "      <td>Western Indo-Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0015772a-fcc3-4bd8-bfb8-2a3b67520f35</td>\n",
       "      <td>983267a0-7349-4d3e-a23e-fb9353ca8ba5</td>\n",
       "      <td>Central Indo-Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001580fa-3324-4053-a74b-c5ef08d49d07</td>\n",
       "      <td>1d31d9ea-e78d-438b-8667-0d63d1aba257</td>\n",
       "      <td>Western Indo-Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8068</th>\n",
       "      <td>fff22808-8720-4a33-9947-f43e3b1ada6b</td>\n",
       "      <td>1d31d9ea-e78d-438b-8667-0d63d1aba257</td>\n",
       "      <td>Western Indo-Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8069</th>\n",
       "      <td>fff84912-4e02-4724-8a90-2443b2b43130</td>\n",
       "      <td>983267a0-7349-4d3e-a23e-fb9353ca8ba5</td>\n",
       "      <td>Central Indo-Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8070</th>\n",
       "      <td>fff8f7ad-2772-43f1-ad66-c65994bea5f2</td>\n",
       "      <td>983267a0-7349-4d3e-a23e-fb9353ca8ba5</td>\n",
       "      <td>Central Indo-Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8071</th>\n",
       "      <td>fffc7eff-cb8d-47b3-9081-a1f6d092e781</td>\n",
       "      <td>1d31d9ea-e78d-438b-8667-0d63d1aba257</td>\n",
       "      <td>Western Indo-Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8072</th>\n",
       "      <td>fffcf3d6-7eb5-471a-8d60-331c9c5ba50d</td>\n",
       "      <td>1d31d9ea-e78d-438b-8667-0d63d1aba257</td>\n",
       "      <td>Western Indo-Pacific</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8073 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  image_id  \\\n",
       "0     00059a47-03b8-47f3-adf6-3ab5616922cf   \n",
       "1     00086e76-2b0d-48ff-a25b-31020c226047   \n",
       "2     00138e67-611c-4e04-a382-46e0484f2f95   \n",
       "3     0015772a-fcc3-4bd8-bfb8-2a3b67520f35   \n",
       "4     001580fa-3324-4053-a74b-c5ef08d49d07   \n",
       "...                                    ...   \n",
       "8068  fff22808-8720-4a33-9947-f43e3b1ada6b   \n",
       "8069  fff84912-4e02-4724-8a90-2443b2b43130   \n",
       "8070  fff8f7ad-2772-43f1-ad66-c65994bea5f2   \n",
       "8071  fffc7eff-cb8d-47b3-9081-a1f6d092e781   \n",
       "8072  fffcf3d6-7eb5-471a-8d60-331c9c5ba50d   \n",
       "\n",
       "                                 region_id           region_name  \n",
       "0     1d31d9ea-e78d-438b-8667-0d63d1aba257  Western Indo-Pacific  \n",
       "1     1d31d9ea-e78d-438b-8667-0d63d1aba257  Western Indo-Pacific  \n",
       "2     1d31d9ea-e78d-438b-8667-0d63d1aba257  Western Indo-Pacific  \n",
       "3     983267a0-7349-4d3e-a23e-fb9353ca8ba5  Central Indo-Pacific  \n",
       "4     1d31d9ea-e78d-438b-8667-0d63d1aba257  Western Indo-Pacific  \n",
       "...                                    ...                   ...  \n",
       "8068  1d31d9ea-e78d-438b-8667-0d63d1aba257  Western Indo-Pacific  \n",
       "8069  983267a0-7349-4d3e-a23e-fb9353ca8ba5  Central Indo-Pacific  \n",
       "8070  983267a0-7349-4d3e-a23e-fb9353ca8ba5  Central Indo-Pacific  \n",
       "8071  1d31d9ea-e78d-438b-8667-0d63d1aba257  Western Indo-Pacific  \n",
       "8072  1d31d9ea-e78d-438b-8667-0d63d1aba257  Western Indo-Pacific  \n",
       "\n",
       "[8073 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict[\"train\"].df_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53eb29f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4e5ab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_s3(\n",
    "    s3,\n",
    "    bucket: str,\n",
    "    key: str,\n",
    "    thumbnail: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetches an image from an S3 bucket and returns it as a PIL Image object.\n",
    "    Args:\n",
    "        s3 (boto3.client): The Boto3 S3 client used to interact with S3.\n",
    "        bucket (str): The name of the S3 bucket.\n",
    "        key (str): The key (path) of the image in the S3 bucket.\n",
    "        thumbnail (bool, optional): If True, fetches the thumbnail version of the image by modifying the key. Defaults to False.\n",
    "    Returns:\n",
    "        PIL.Image.Image: The image loaded from S3 as a PIL Image object.\n",
    "    \"\"\"\n",
    "\n",
    "    if thumbnail:\n",
    "        key = key.replace(\".png\", \"_thumbnail.png\")\n",
    "\n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    image_data = response[\"Body\"].read()\n",
    "\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    return image\n",
    "\n",
    "def create_annotation_mask(\n",
    "    annotations,\n",
    "    shape,\n",
    "    label2id,\n",
    "    padding = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creates an annotation mask for a given image based on provided annotations.\n",
    "    Args:\n",
    "        annotations (pd.DataFrame): DataFrame containing annotation rows with 'row', 'col', and 'benthic_attribute_name' columns.\n",
    "        shape (Tuple[int, int]): Shape of the output mask (height, width).\n",
    "        label2id (Dict[str, int]): Mapping from label names to integer IDs.\n",
    "    Returns:\n",
    "        np.ndarray: Annotation mask with integer class IDs.\n",
    "    \"\"\"\n",
    "    ## TODO: Make Padding percentage based so that it is applicable to all class sizes\n",
    "    mask = np.zeros(shape[:2])\n",
    "    for _, annotation in annotations.iterrows():\n",
    "        if annotation[\"benthic_attribute_name\"] is not None:\n",
    "            if padding is not None and padding > 0:\n",
    "                mask[\n",
    "                    annotation[\"row\"] - padding : annotation[\"row\"] + padding,\n",
    "                    annotation[\"col\"] - padding : annotation[\"col\"] + padding,\n",
    "                ] = label2id[annotation[\"benthic_attribute_name\"]]\n",
    "            else:\n",
    "                mask[annotation[\"row\"], annotation[\"col\"]] = label2id[\n",
    "                    annotation[\"benthic_attribute_name\"]\n",
    "                ]\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1de3957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(self, image_id: str, **row_kwargs):\n",
    "    \"\"\"\n",
    "    Read an image given its ID. Needs to be implemented in subclasses.\n",
    "    \"\"\"\n",
    "    key = f\"mermaid/{image_id}.png\"  # f\"mermaid/{image_id}_thumbnail.png\"\n",
    "    image = np.array(\n",
    "        get_image_s3(s3=self.s3, bucket=self.source_bucket, key=key).convert(\"RGB\")\n",
    "    )\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c46f5599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Dataset_tmp1(Dataset):\n",
    "    def __init__(self):\n",
    "        self.df_images = dataset_dict[\"train\"].df_images\n",
    "        self.transform = dataset_dict[\"train\"].transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_images)\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self.df_images.loc[idx, \"image_id\"]\n",
    "        row_kwargs = self.df_images.loc[idx].to_dict()\n",
    "\n",
    "        image = dataset_dict[\"train\"].read_image(**row_kwargs)\n",
    "\n",
    "        annotations = dataset_dict[\"train\"].df_annotations.loc[\n",
    "            dataset_dict[\"train\"].df_annotations[\"image_id\"] == image_id,\n",
    "            [\n",
    "                \"row\",\n",
    "                \"col\",\n",
    "                \"benthic_attribute_name\",\n",
    "            ],\n",
    "        ]\n",
    "\n",
    "        mask = create_annotation_mask(\n",
    "            annotations, image.shape, dataset_dict[\"train\"].label2id, padding=dataset_dict[\"train\"].padding\n",
    "    )\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"].transpose(2, 0, 1)\n",
    "            mask = transformed[\"mask\"]\n",
    "\n",
    "        return image, mask, annotations\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Collate function for MermaidDataset and CoralNetDataset.\n",
    "        Args:\n",
    "            batch: List of tuples (image, mask, annotations)\n",
    "        Returns:\n",
    "            images: Tensor or ndarray batch of images\n",
    "            masks: Tensor or ndarray batch of masks\n",
    "            annotations: List of annotation DataFrames\n",
    "        \"\"\"\n",
    "        # images, masks, annotations = zip(*batch)\n",
    "\n",
    "        # Filter out entries where image or mask is None\n",
    "        filtered = [\n",
    "            (img, msk, ann)\n",
    "            for img, msk, ann in batch\n",
    "            if img is not None and msk is not None\n",
    "        ]\n",
    "        images, masks, annotations = zip(*filtered)\n",
    "\n",
    "        # Handle empty batch\n",
    "        if len(images) == 0:\n",
    "            return torch.tensor([]), torch.tensor([]), []\n",
    "\n",
    "        # Convert to tensors if they aren't already\n",
    "        if isinstance(images[0], torch.Tensor):\n",
    "            images = torch.stack(images)\n",
    "            masks = torch.stack(masks)\n",
    "        else:\n",
    "            # Convert numpy arrays to tensors for consistency\n",
    "            images = torch.stack(\n",
    "                [\n",
    "                    torch.from_numpy(img) if isinstance(img, np.ndarray) else img\n",
    "                    for img in images\n",
    "                ]\n",
    "            )\n",
    "            masks = torch.stack(\n",
    "                [\n",
    "                    torch.from_numpy(mask) if isinstance(mask, np.ndarray) else mask\n",
    "                    for mask in masks\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ae0773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:10<04:00,  5.01s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 14.56 GiB of which 9.75 MiB is free. Process 51245 has 4.42 GiB memory in use. Process 72870 has 4.66 GiB memory in use. Process 93954 has 3.48 GiB memory in use. Including non-PyTorch memory, this process has 1.99 GiB memory in use. Of the allocated memory 1.67 GiB is allocated by PyTorch, and 202.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(meta_model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     13\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 14\u001b[0m loss, outputs, concept_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmeta_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_predict_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m time_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtypical_test_2\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\u001b[38;5;241m/\u001b[39mnum_iterations\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m loss, outputs, concept_outputs\n",
      "File \u001b[0;32m~/mermaid-segmentation/mermaidseg/model/meta.py:260\u001b[0m, in \u001b[0;36mMetaModel.batch_predict_loss\u001b[0;34m(self, batch, target_dim)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcept\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    259\u001b[0m     concept_outputs \u001b[38;5;241m=\u001b[39m segmentation_outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m--> 260\u001b[0m     concept_labels \u001b[38;5;241m=\u001b[39m \u001b[43mlabels_to_concepts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcept_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(concept_outputs, concept_labels, labels)\n\u001b[1;32m    262\u001b[0m     concept_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(concept_outputs)\n",
      "File \u001b[0;32m~/mermaid-segmentation/mermaidseg/datasets/concepts.py:244\u001b[0m, in \u001b[0;36mlabels_to_concepts\u001b[0;34m(labels, benthic_concept_matrix)\u001b[0m\n\u001b[1;32m    242\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m lookup[labels_np]  \u001b[38;5;66;03m# shape (B, H, W, C)\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtranspose(mapped, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# (B, C, H, W)\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapped\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# numpy input\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 14.56 GiB of which 9.75 MiB is free. Process 51245 has 4.42 GiB memory in use. Process 72870 has 4.66 GiB memory in use. Process 93954 has 3.48 GiB memory in use. Including non-PyTorch memory, this process has 1.99 GiB memory in use. Of the allocated memory 1.67 GiB is allocated by PyTorch, and 202.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "dataset_tmp1 = Dataset_tmp1()\n",
    "train_loader = DataLoader(dataset_tmp1, batch_size=batch_size, shuffle=True, num_workers=1, drop_last=True, collate_fn = dataset_tmp1.collate_fn)\n",
    "time_dict[\"typical_test_2\"] = {\"data_io\":0, \"forward\":0}\n",
    "num_iterations = 50\n",
    "for i in tqdm.tqdm(range(num_iterations)):\n",
    "        start_time = time.time()\n",
    "        data = next(iter(train_loader))\n",
    "        time_dict[\"typical_test_2\"][\"data_io\"] += (time.time() - start_time)/num_iterations\n",
    "\n",
    "        _, labels = data\n",
    "        labels = labels.long().to(meta_model.device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss, outputs, concept_outputs = meta_model.batch_predict_loss(data)\n",
    "        time_dict[\"typical_test_2\"][\"forward\"] += (time.time() - start_time)/num_iterations\n",
    "        \n",
    "        del loss, outputs, concept_outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8abf82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_io': 1.83486671924591, 'forward': 0.7137363386154176}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_dict[\"typical_test_2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751354c4",
   "metadata": {},
   "source": [
    "### Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8cbc5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# for idx in range(100):    \n",
    "#         image_id = dataset_tmp1.df_images.loc[idx, \"image_id\"]\n",
    "#         row_kwargs = dataset_tmp1.df_images.loc[idx].to_dict()\n",
    "\n",
    "#         image = dataset_dict[\"train\"].read_image(**row_kwargs)\n",
    "#         out_dir = \"../data_images\"\n",
    "#         os.makedirs(out_dir, exist_ok=True)\n",
    "#         out_path = os.path.join(out_dir, f\"image_{idx}.png\")\n",
    "#         Image.fromarray(image).save(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "676ac57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_tmp2(Dataset):\n",
    "    def __init__(self):\n",
    "        self.df_images = dataset_dict[\"train\"].df_images[:100]\n",
    "        self.transform = dataset_dict[\"train\"].transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_images)\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self.df_images.loc[idx, \"image_id\"]\n",
    "        row_kwargs = self.df_images.loc[idx].to_dict()\n",
    "\n",
    "        image_path = os.path.join(\"../data_images\", f\"image_{idx}.png\")\n",
    "        image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "\n",
    "        annotations = dataset_dict[\"train\"].df_annotations.loc[\n",
    "            dataset_dict[\"train\"].df_annotations[\"image_id\"] == image_id,\n",
    "            [\"row\", \"col\", \"benthic_attribute_name\"],\n",
    "        ]\n",
    "\n",
    "        mask = create_annotation_mask(\n",
    "            annotations, image.shape, dataset_dict[\"train\"].label2id, padding=dataset_dict[\"train\"].padding\n",
    "    )\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"].transpose(2, 0, 1)\n",
    "            mask = transformed[\"mask\"]\n",
    "\n",
    "        return image, mask, annotations\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Collate function for MermaidDataset and CoralNetDataset.\n",
    "        Args:\n",
    "            batch: List of tuples (image, mask, annotations)\n",
    "        Returns:\n",
    "            images: Tensor or ndarray batch of images\n",
    "            masks: Tensor or ndarray batch of masks\n",
    "            annotations: List of annotation DataFrames\n",
    "        \"\"\"\n",
    "        # images, masks, annotations = zip(*batch)\n",
    "\n",
    "        # Filter out entries where image or mask is None\n",
    "        filtered = [\n",
    "            (img, msk, ann)\n",
    "            for img, msk, ann in batch\n",
    "            if img is not None and msk is not None\n",
    "        ]\n",
    "        images, masks, annotations = zip(*filtered)\n",
    "\n",
    "        # Handle empty batch\n",
    "        if len(images) == 0:\n",
    "            return torch.tensor([]), torch.tensor([]), []\n",
    "\n",
    "        # Convert to tensors if they aren't already\n",
    "        if isinstance(images[0], torch.Tensor):\n",
    "            images = torch.stack(images)\n",
    "            masks = torch.stack(masks)\n",
    "        else:\n",
    "            # Convert numpy arrays to tensors for consistency\n",
    "            images = torch.stack(\n",
    "                [\n",
    "                    torch.from_numpy(img) if isinstance(img, np.ndarray) else img\n",
    "                    for img in images\n",
    "                ]\n",
    "            )\n",
    "            masks = torch.stack(\n",
    "                [\n",
    "                    torch.from_numpy(mask) if isinstance(mask, np.ndarray) else mask\n",
    "                    for mask in masks\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "be6cbebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:37<00:00,  3.16s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_tmp2 = Dataset_tmp2()\n",
    "train_loader = DataLoader(dataset_tmp2, batch_size=batch_size, shuffle=True, num_workers=1, drop_last=True, collate_fn = dataset_tmp2.collate_fn)\n",
    "time_dict[\"local_test\"] = {\"data_io\":0, \"forward\":0}\n",
    "num_iterations = 50\n",
    "for i in tqdm.tqdm(range(num_iterations)):\n",
    "        start_time = time.time()\n",
    "        data = next(iter(train_loader))\n",
    "        time_dict[\"local_test\"][\"data_io\"] += (time.time() - start_time)/num_iterations\n",
    "\n",
    "        _, labels = data\n",
    "        labels = labels.long().to(meta_model.device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss, outputs, concept_outputs = meta_model.batch_predict_loss(data)\n",
    "        time_dict[\"local_test\"][\"forward\"] += (time.time() - start_time)/num_iterations\n",
    "        \n",
    "        del loss, outputs, concept_outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "61993ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_io': 2.348804125785827, 'forward': 0.7396216869354246}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_dict[\"local_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "779e3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:49<00:00,  3.38s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_tmp2 = Dataset_tmp2()\n",
    "train_loader = DataLoader(dataset_tmp2, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True, collate_fn = dataset_tmp2.collate_fn)\n",
    "time_dict[\"local_test_num_workers_2\"] = {\"data_io\":0, \"forward\":0}\n",
    "num_iterations = 50\n",
    "for i in tqdm.tqdm(range(num_iterations)):\n",
    "        start_time = time.time()\n",
    "        data = next(iter(train_loader))\n",
    "        time_dict[\"local_test_num_workers_2\"][\"data_io\"] += (time.time() - start_time)/num_iterations\n",
    "\n",
    "        _, labels = data\n",
    "        labels = labels.long().to(meta_model.device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss, outputs, concept_outputs = meta_model.batch_predict_loss(data)\n",
    "        time_dict[\"local_test_num_workers_2\"][\"forward\"] += (time.time() - start_time)/num_iterations\n",
    "        \n",
    "        del loss, outputs, concept_outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "596dbe11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_io': 2.576789755821229, 'forward': 0.7402181720733643}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_dict[\"local_test_num_workers_2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462ae52",
   "metadata": {},
   "source": [
    "## No Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a20e19d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_tmp3(Dataset):\n",
    "    def __init__(self):\n",
    "        self.df_images = dataset_dict[\"train\"].df_images[:100]\n",
    "        self.transform = dataset_dict[\"train\"].transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_images)\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self.df_images.loc[idx, \"image_id\"]\n",
    "        row_kwargs = self.df_images.loc[idx].to_dict()\n",
    "\n",
    "        image = dataset_dict[\"train\"].read_image(**row_kwargs)\n",
    "\n",
    "        annotations = []\n",
    "        # annotations = dataset_dict[\"train\"].df_annotations.loc[\n",
    "        #     dataset_dict[\"train\"].df_annotations[\"image_id\"] == image_id,\n",
    "        #     [\"row\", \"col\", \"benthic_attribute_name\"],\n",
    "        # ]\n",
    "\n",
    "        mask = np.zeros(image.shape[:2], dtype = np.uint8)\n",
    "        # mask = create_annotation_mask(\n",
    "        #     annotations, image.shape, dataset_dict[\"train\"].label2id, padding=dataset_dict[\"train\"].padding\n",
    "        # )\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"].transpose(2, 0, 1)\n",
    "            mask = transformed[\"mask\"]\n",
    "\n",
    "        return image, mask, annotations\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Collate function for MermaidDataset and CoralNetDataset.\n",
    "        Args:\n",
    "            batch: List of tuples (image, mask, annotations)\n",
    "        Returns:\n",
    "            images: Tensor or ndarray batch of images\n",
    "            masks: Tensor or ndarray batch of masks\n",
    "            annotations: List of annotation DataFrames\n",
    "        \"\"\"\n",
    "        # images, masks, annotations = zip(*batch)\n",
    "\n",
    "        # Filter out entries where image or mask is None\n",
    "        filtered = [\n",
    "            (img, msk, ann)\n",
    "            for img, msk, ann in batch\n",
    "            if img is not None and msk is not None\n",
    "        ]\n",
    "        images, masks, annotations = zip(*filtered)\n",
    "    \n",
    "        # Handle empty batch\n",
    "        if len(images) == 0:\n",
    "            return torch.tensor([]), torch.tensor([]), []\n",
    "\n",
    "        # Convert to tensors if they aren't already\n",
    "        if isinstance(images[0], torch.Tensor):\n",
    "            images = torch.stack(images)\n",
    "            masks = torch.stack(masks)\n",
    "        else:\n",
    "            # Convert numpy arrays to tensors for consistency\n",
    "            images = torch.stack(\n",
    "                [\n",
    "                    torch.from_numpy(img) if isinstance(img, np.ndarray) else img\n",
    "                    for img in images\n",
    "                ]\n",
    "            )\n",
    "            masks = torch.stack(\n",
    "                [\n",
    "                    torch.from_numpy(mask) if isinstance(mask, np.ndarray) else mask\n",
    "                    for mask in masks\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c69b212f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:56<00:00,  2.32s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_tmp3 = Dataset_tmp3()\n",
    "train_loader = DataLoader(dataset_tmp3, batch_size=batch_size, shuffle=True, num_workers=1, drop_last=True, collate_fn = dataset_tmp3.collate_fn)\n",
    "time_dict[\"no_transform\"] = {\"data_io\":0, \"forward\":0}\n",
    "num_iterations = 50\n",
    "for i in tqdm.tqdm(range(num_iterations)):\n",
    "        start_time = time.time()\n",
    "        data = next(iter(train_loader))\n",
    "        time_dict[\"no_transform\"][\"data_io\"] += (time.time() - start_time)/num_iterations\n",
    "\n",
    "        _, labels = data\n",
    "        labels = labels.long().to(meta_model.device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss, outputs, concept_outputs = meta_model.batch_predict_loss(data)\n",
    "        time_dict[\"no_transform\"][\"forward\"] += (time.time() - start_time)/num_iterations\n",
    "        \n",
    "        del loss, outputs, concept_outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "10219399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_io': 1.7039057350158688, 'forward': 0.6439124155044555}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_dict[\"no_transform\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d0169",
   "metadata": {},
   "source": [
    "# Improve collation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cbbb2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_tmp4(Dataset):\n",
    "    def __init__(self):\n",
    "        self.df_images = dataset_dict[\"train\"].df_images[:100]\n",
    "        self.transform = dataset_dict[\"train\"].transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_images)\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self.df_images.loc[idx, \"image_id\"]\n",
    "        row_kwargs = self.df_images.loc[idx].to_dict()\n",
    "\n",
    "        image = dataset_dict[\"train\"].read_image(**row_kwargs)\n",
    "\n",
    "        annotations = dataset_dict[\"train\"].df_annotations.loc[\n",
    "            dataset_dict[\"train\"].df_annotations[\"image_id\"] == image_id,\n",
    "            [\"row\", \"col\", \"benthic_attribute_name\"],\n",
    "        ]\n",
    "\n",
    "        mask = np.zeros(image.shape[:2], dtype = np.uint8)\n",
    "        mask = create_annotation_mask(\n",
    "            annotations, image.shape, dataset_dict[\"train\"].label2id, padding=dataset_dict[\"train\"].padding\n",
    "        )\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"].transpose(2, 0, 1)\n",
    "            mask = transformed[\"mask\"]\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    # def collate_fn(self, batch):\n",
    "    #     \"\"\"\n",
    "    #     Collate function for MermaidDataset and CoralNetDataset.\n",
    "    #     Args:\n",
    "    #         batch: List of tuples (image, mask, annotations)\n",
    "    #     Returns:\n",
    "    #         images: Tensor or ndarray batch of images\n",
    "    #         masks: Tensor or ndarray batch of masks\n",
    "    #         annotations: List of annotation DataFrames\n",
    "    #     \"\"\"\n",
    "    #     # images, masks, annotations = zip(*batch)\n",
    "\n",
    "    #     # Filter out entries where image or mask is None\n",
    "    #     # filtered = [\n",
    "    #     #     (img, msk, ann)\n",
    "    #     #     for img, msk, ann in batch\n",
    "    #     #     if img is not None and msk is not None\n",
    "    #     # ]\n",
    "    #     # images, masks, annotations = zip(*filtered)\n",
    "    #     images, masks, annotations = zip(*batch)\n",
    "    #     # Handle empty batch\n",
    "    #     if len(images) == 0:\n",
    "    #         return torch.tensor([]), torch.tensor([]), []\n",
    "\n",
    "    #     # Convert to tensors if they aren't already\n",
    "    #     if isinstance(images[0], torch.Tensor):\n",
    "    #         images = torch.stack(images)\n",
    "    #         masks = torch.stack(masks)\n",
    "    #     else:\n",
    "    #         # Convert numpy arrays to tensors for consistency\n",
    "    #         images = torch.stack(\n",
    "    #             [\n",
    "    #                 torch.from_numpy(img) if isinstance(img, np.ndarray) else img\n",
    "    #                 for img in images\n",
    "    #             ]\n",
    "    #         )\n",
    "    #         masks = torch.stack(\n",
    "    #             [\n",
    "    #                 torch.from_numpy(mask) if isinstance(mask, np.ndarray) else mask\n",
    "    #                 for mask in masks\n",
    "    #             ]\n",
    "    #         )\n",
    "\n",
    "    #     return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1a3473ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:03<00:00,  2.47s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_tmp4 = Dataset_tmp4()\n",
    "train_loader = DataLoader(dataset_tmp4, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
    "time_dict[\"collation_1\"] = {\"data_io\":0, \"forward\":0}\n",
    "num_iterations = 50\n",
    "for i in tqdm.tqdm(range(num_iterations)):\n",
    "        start_time = time.time()\n",
    "        data = next(iter(train_loader))\n",
    "        time_dict[\"collation_1\"][\"data_io\"] += (time.time() - start_time)/num_iterations\n",
    "\n",
    "        _, labels = data\n",
    "        labels = labels.long().to(meta_model.device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss, outputs, concept_outputs = meta_model.batch_predict_loss(data)\n",
    "        time_dict[\"collation_1\"][\"forward\"] += (time.time() - start_time)/num_iterations\n",
    "        \n",
    "        del loss, outputs, concept_outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "81f74537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:32<00:00,  3.12it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx in tqdm.tqdm(range(100)):\n",
    "    image_id = dataset_tmp4.df_images.loc[idx, \"image_id\"]\n",
    "    row_kwargs = dataset_tmp4.df_images.loc[idx].to_dict()\n",
    "    image = dataset_dict[\"train\"].read_image(**row_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a1f540",
   "metadata": {},
   "source": [
    "## No Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e4c9e9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_tmp5(Dataset):\n",
    "    def __init__(self):\n",
    "        self.df_images = dataset_dict[\"train\"].df_images[:100]\n",
    "        self.transform = dataset_dict[\"train\"].transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df_images)\n",
    "    def __getitem__(self, idx: int):\n",
    "        image_id = self.df_images.loc[idx, \"image_id\"]\n",
    "        row_kwargs = self.df_images.loc[idx].to_dict()\n",
    "\n",
    "        image = dataset_dict[\"train\"].read_image(**row_kwargs)\n",
    "\n",
    "        annotations = dataset_dict[\"train\"].df_annotations.loc[\n",
    "            dataset_dict[\"train\"].df_annotations[\"image_id\"] == image_id,\n",
    "            [\"row\", \"col\", \"benthic_attribute_name\"],\n",
    "        ]\n",
    "\n",
    "        mask = create_annotation_mask(\n",
    "            annotations, image.shape, dataset_dict[\"train\"].label2id, padding=dataset_dict[\"train\"].padding\n",
    "        )\n",
    "\n",
    "        image = image[:512, :512].transpose(2, 0, 1)\n",
    "        mask = mask[:512, :512]\n",
    "        # if self.transform:\n",
    "        #     transformed = self.transform(image=image, mask=mask)\n",
    "        #     image = transformed[\"image\"].transpose(2, 0, 1)\n",
    "        #     mask = transformed[\"mask\"]\n",
    "\n",
    "        return image, mask, annotations\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Collate function for MermaidDataset and CoralNetDataset.\n",
    "        Args:\n",
    "            batch: List of tuples (image, mask, annotations)\n",
    "        Returns:\n",
    "            images: Tensor or ndarray batch of images\n",
    "            masks: Tensor or ndarray batch of masks\n",
    "            annotations: List of annotation DataFrames\n",
    "        \"\"\"\n",
    "        # images, masks, annotations = zip(*batch)\n",
    "\n",
    "        # Filter out entries where image or mask is None\n",
    "        filtered = [\n",
    "            (img, msk, ann)\n",
    "            for img, msk, ann in batch\n",
    "            if img is not None and msk is not None\n",
    "        ]\n",
    "        images, masks, annotations = zip(*filtered)\n",
    "        # Handle empty batch\n",
    "        if len(images) == 0:\n",
    "            return torch.tensor([]), torch.tensor([]), []\n",
    "\n",
    "        # Convert to tensors if they aren't already\n",
    "        if isinstance(images[0], torch.Tensor):\n",
    "            images = torch.stack(images)\n",
    "            masks = torch.stack(masks)\n",
    "        else:\n",
    "            # Convert numpy arrays to tensors for consistency\n",
    "            images = torch.stack(\n",
    "                [\n",
    "                    torch.from_numpy(img) if isinstance(img, np.ndarray) else img\n",
    "                    for img in images\n",
    "                ]\n",
    "            )\n",
    "            masks = torch.stack(\n",
    "                [\n",
    "                    torch.from_numpy(mask) if isinstance(mask, np.ndarray) else mask\n",
    "                    for mask in masks\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return images, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3b6ebc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:03<00:00,  2.47s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_tmp5 = Dataset_tmp5()\n",
    "train_loader = DataLoader(dataset_tmp5, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True, collate_fn = dataset_tmp5.collate_fn)\n",
    "time_dict[\"no_transform\"] = {\"data_io\":0, \"forward\":0}\n",
    "num_iterations = 50\n",
    "for i in tqdm.tqdm(range(num_iterations)):\n",
    "        start_time = time.time()\n",
    "        data = next(iter(train_loader))\n",
    "        time_dict[\"no_transform\"][\"data_io\"] += (time.time() - start_time)/num_iterations\n",
    "\n",
    "        _, labels = data\n",
    "        labels = labels.long().to(meta_model.device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss, outputs, concept_outputs = meta_model.batch_predict_loss(data)\n",
    "        time_dict[\"no_transform\"][\"forward\"] += (time.time() - start_time)/num_iterations\n",
    "        \n",
    "        del loss, outputs, concept_outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d42a8d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_io': 1.69118703365326, 'forward': 0.7109530687332154}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_dict[\"no_transform\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89d15d",
   "metadata": {},
   "source": [
    "## Batch Size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d210b712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [05:04<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=1, drop_last=True, collate_fn = dataset_dict[\"train\"].collate_fn)\n",
    "\n",
    "time_dict[\"batch_size_1\"] = {\"data_io\":0, \"forward\":0}\n",
    "num_iterations = 100*(batch_size//1)\n",
    "for i in tqdm.tqdm(range(num_iterations)):\n",
    "        start_time = time.time()\n",
    "        data = next(iter(train_loader))\n",
    "        time_dict[\"batch_size_1\"][\"data_io\"] += (time.time() - start_time)/num_iterations\n",
    "\n",
    "        _, labels = data\n",
    "        labels = labels.long().to(meta_model.device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss, outputs, concept_outputs = meta_model.batch_predict_loss(data)\n",
    "        time_dict[\"batch_size_1\"][\"forward\"] += (time.time() - start_time)/num_iterations\n",
    "        \n",
    "        del loss, outputs, concept_outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b5392945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_io': 2.210755922794341, 'forward': 0.7570949435234068}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v*(batch_size//1) for k,v in time_dict[\"batch_size_1\"].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e65019a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:18<00:00,  5.17s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=1, drop_last=True, collate_fn = dataset_dict[\"train\"].collate_fn)\n",
    "\n",
    "time_dict[\"batch_size_4\"] = {\"data_io\":0, \"forward\":0}\n",
    "num_iterations = 100*batch_size//4\n",
    "for i in tqdm.tqdm(range(num_iterations)):\n",
    "        start_time = time.time()\n",
    "        data = next(iter(train_loader))\n",
    "        time_dict[\"batch_size_4\"][\"data_io\"] += (time.time() - start_time)/num_iterations\n",
    "\n",
    "        _, labels = data\n",
    "        labels = labels.long().to(meta_model.device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss, outputs, concept_outputs = meta_model.batch_predict_loss(data)\n",
    "        time_dict[\"batch_size_4\"][\"forward\"] += (time.time() - start_time)/num_iterations\n",
    "        \n",
    "        del loss, outputs, concept_outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "18cc6d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_io': 1.7883255410194396, 'forward': 0.7358332681655884}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v*batch_size/4 for k,v in time_dict[\"batch_size_4\"].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b50c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [04:37<00:05,  5.80s/it]"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, drop_last=True, collate_fn = dataset_dict[\"train\"].collate_fn)\n",
    "\n",
    "time_dict[\"batch_size_4_nw4\"] = {\"data_io\":0, \"forward\":0}\n",
    "num_iterations = 100*batch_size//4\n",
    "for i in tqdm.tqdm(range(num_iterations)):\n",
    "        start_time = time.time()\n",
    "        data = next(iter(train_loader))\n",
    "        time_dict[\"batch_size_4_nw4\"][\"data_io\"] += (time.time() - start_time)/num_iterations\n",
    "\n",
    "        _, labels = data\n",
    "        labels = labels.long().to(meta_model.device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss, outputs, concept_outputs = meta_model.batch_predict_loss(data)\n",
    "        time_dict[\"batch_size_4_nw4\"][\"forward\"] += (time.time() - start_time)/num_iterations\n",
    "        \n",
    "        del loss, outputs, concept_outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b067899",
   "metadata": {},
   "outputs": [],
   "source": [
    "{k:v*batch_size/4 for k,v in time_dict[\"batch_size_4_nw4\"].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9b6118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:25<00:00,  7.72s/it]\n"
     ]
    }
   ],
   "source": [
    "dataset_tmp2 = Dataset_tmp2()\n",
    "train_loader = DataLoader(dataset_tmp2, batch_size=4, shuffle=True, num_workers=4, drop_last=True, collate_fn = dataset_tmp2.collate_fn)\n",
    "time_dict[\"local_test_batch_size_4\"] = {\"data_io\":0, \"forward\":0}\n",
    "num_iterations = 100*batch_size//4\n",
    "for i in tqdm.tqdm(range(num_iterations)):\n",
    "        start_time = time.time()\n",
    "        data = next(iter(train_loader))\n",
    "        time_dict[\"local_test_batch_size_4\"][\"data_io\"] += (time.time() - start_time)/num_iterations\n",
    "\n",
    "        _, labels = data\n",
    "        labels = labels.long().to(meta_model.device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss, outputs, concept_outputs = meta_model.batch_predict_loss(data)\n",
    "        time_dict[\"local_test_batch_size_4\"][\"forward\"] += (time.time() - start_time)/num_iterations\n",
    "        \n",
    "        del loss, outputs, concept_outputs\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bf3e55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_io': 3.0653994894027714, 'forward': 0.7343459630012513}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:v*batch_size/4 for k,v in time_dict[\"local_test_batch_size_4\"].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c8ccf6-63d8-4a43-abfd-d3a63d9a2483",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
