{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb01347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import tqdm\n",
    "\n",
    "def check_s3_prefix_exists(bucket_name, s3_prefix, source_id, specific_file = \"annotations.csv\"):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    prefix = f\"{s3_prefix}/s{source_id}/{specific_file}\"\n",
    "\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix, MaxKeys=1)\n",
    "\n",
    "    if \"Contents\" in response:\n",
    "        # print(f\"Prefix exists: {prefix}\")\n",
    "        return True\n",
    "    else:\n",
    "        # print(f\"Prefix does not exist: {prefix}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bacdfb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1796 links on the page.\n",
      "Found 1786 links on the page.\n"
     ]
    }
   ],
   "source": [
    "url = \"https://coralnet.ucsd.edu/source/about/\"\n",
    "\n",
    "resp = requests.get(url, timeout=50)\n",
    "resp.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "anchors = soup.find_all(\"a\", href=True)\n",
    "\n",
    "links = sorted(\n",
    "    {\n",
    "        urljoin(url, a[\"href\"])\n",
    "        for a in anchors\n",
    "        if urlparse(urljoin(url, a[\"href\"])).scheme in (\"http\", \"https\")\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Found\", len(links), \"links on the page.\")\n",
    "source_links = [link for link in links if \"/source/\" in link]\n",
    "print(\"Found\", len(source_links), \"links on the page.\")\n",
    "all_coralnet_sources = sorted({int(link.split(\"/\")[-2]) for link in source_links})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3c4c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Where to save the downloaded CoralNet images\n",
    "bucket_name = \"dev-datamermaid-sm-sources\"\n",
    "prefix = \"coralnet-public-images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa4f5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1786it [07:34,  3.93it/s]\n"
     ]
    }
   ],
   "source": [
    "data = [] \n",
    "for i, source_id in tqdm.tqdm(enumerate(all_coralnet_sources)):\n",
    "    # print(i, \"Source ID\", source_id)\n",
    "    labelset_flag = check_s3_prefix_exists(\n",
    "        bucket_name=bucket_name, s3_prefix=prefix, source_id=source_id, specific_file=\"labelset.csv\"\n",
    "    )\n",
    "    metadata_flag = check_s3_prefix_exists(\n",
    "        bucket_name=bucket_name, s3_prefix=prefix, source_id=source_id, specific_file=\"metadata.csv\"\n",
    "    )\n",
    "    annotations_flag = check_s3_prefix_exists(\n",
    "        bucket_name=bucket_name, s3_prefix=prefix, source_id=source_id, specific_file=\"annotations.csv\"\n",
    "    )\n",
    "    image_list_flag = check_s3_prefix_exists(\n",
    "        bucket_name=bucket_name, s3_prefix=prefix, source_id=source_id, specific_file=\"image_list.csv\"\n",
    "    )\n",
    "    data.append([i, source_id, labelset_flag, metadata_flag, annotations_flag, image_list_flag]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70b79b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "source_status = pd.DataFrame(data, columns = [\"idx\", \"source_id\", \"labelset\", \"metadata\", \"annotations\", \"image_list\"])\n",
    "source_status.head()\n",
    "source_ids = source_status[(source_status[\"annotations\"]==True)*(source_status[\"image_list\"]==True)][\"source_id\"].values\n",
    "source_ids_start = source_ids[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b212a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  23,   57,   69,   70,   82,  109,  155,  172,  173,  174,  258,\n",
       "        290,  299,  307,  350,  428,  450,  466,  503,  546,  580,  616,\n",
       "        620,  648,  683,  747,  793,  800,  841,  842,  843,  921,  958,\n",
       "       1073, 1076, 1162, 1184, 1189, 1212, 1264, 1265, 1266, 1268, 1269,\n",
       "       1270, 1271, 1272, 1273, 1274, 1276])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_ids_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e71f9b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_bucket = \"dev-datamermaid-sm-sources\"\n",
    "source_s3_prefix = \"coralnet-public-images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e7e49e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63f7b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4b616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:01,  3.15it/s]/tmp/ipykernel_22089/861094551.py:53: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_annotations = pd.concat(\n",
      "6it [00:04,  1.04s/it]/tmp/ipykernel_22089/861094551.py:53: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_annotations = pd.concat(\n",
      "13it [00:14,  1.27s/it]/tmp/ipykernel_22089/861094551.py:30: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_tmp = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
      "15it [00:18,  1.46s/it]/tmp/ipykernel_22089/861094551.py:53: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_annotations = pd.concat(\n",
      "24it [00:42,  2.93s/it]/tmp/ipykernel_22089/861094551.py:30: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_tmp = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
      "26it [00:50,  3.40s/it]/tmp/ipykernel_22089/861094551.py:30: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_tmp = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
      "27it [00:55,  3.98s/it]/tmp/ipykernel_22089/861094551.py:30: DtypeWarning: Columns (2,3,4,5,6,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_tmp = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
      "33it [01:31,  5.65s/it]/tmp/ipykernel_22089/861094551.py:30: DtypeWarning: Columns (10,11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_tmp = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
      "37it [01:52,  5.11s/it]/tmp/ipykernel_22089/861094551.py:53: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_annotations = pd.concat(\n",
      "50it [03:58,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (seconds): 238.16918087005615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# start_time = time.time()\n",
    "# for i, source_id in tqdm.tqdm(enumerate(source_ids_start)):\n",
    "#     if i == 0:\n",
    "#         df_annotations = pd.read_csv(\n",
    "#             f\"s3://{source_bucket}/{source_s3_prefix}/s{source_id}/annotations.csv\"\n",
    "#         )\n",
    "#         df_images = pd.read_csv(\n",
    "#             f\"s3://{source_bucket}/{source_s3_prefix}/s{source_id}/image_list.csv\"  # Perhaps this is unnecessary and can just use tha annotations as in Mermaid\n",
    "#         )\n",
    "#         df_images[\"Name\"] = df_images[\"Name\"].apply(\n",
    "#             lambda x: x.replace(\" - Confirmed\", \"\")\n",
    "#         )\n",
    "#         df_images[\"image_id\"] = df_images[\"Image Page\"].apply(\n",
    "#             lambda x: x.replace(\"/image/\", \"\").replace(\"/view/\", \"\")\n",
    "#         )\n",
    "#         df_annotations = pd.merge(\n",
    "#             df_annotations,\n",
    "#             df_images,\n",
    "#             left_on=\"Name\",\n",
    "#             right_on=\"Name\",\n",
    "#             how=\"left\",\n",
    "#             suffixes=(\"\", \"_y\"),\n",
    "#         )\n",
    "#         df_annotations[\"source_id\"] = source_id\n",
    "#     else:\n",
    "#         obj = s3.get_object(\n",
    "#             Bucket=source_bucket,\n",
    "#             Key=f\"{source_s3_prefix}/s{source_id}/annotations.csv\",\n",
    "#         )\n",
    "#         df_tmp = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "\n",
    "#         obj = s3.get_object(\n",
    "#             Bucket=source_bucket,\n",
    "#             Key=f\"{source_s3_prefix}/s{source_id}/image_list.csv\",\n",
    "#         )\n",
    "#         df_images = pd.read_csv(io.BytesIO(obj[\"Body\"].read()))\n",
    "\n",
    "#         df_images[\"Name\"] = df_images[\"Name\"].apply(\n",
    "#             lambda x: x.replace(\" - Confirmed\", \"\")\n",
    "#         )\n",
    "#         df_images[\"image_id\"] = df_images[\"Image Page\"].apply(\n",
    "#             lambda x: x.replace(\"/image/\", \"\").replace(\"/view/\", \"\")\n",
    "#         )\n",
    "#         df_tmp = pd.merge(\n",
    "#             df_tmp,\n",
    "#             df_images,\n",
    "#             left_on=\"Name\",\n",
    "#             right_on=\"Name\",\n",
    "#             how=\"left\",\n",
    "#             suffixes=(\"\", \"_y\"),\n",
    "#         )\n",
    "#         df_tmp[\"source_id\"] = source_id\n",
    "#         df_annotations = pd.concat(\n",
    "#             [df_annotations, df_tmp], ignore_index=True\n",
    "#         )\n",
    "# end_time = time.time()\n",
    "# print(\"Time taken (seconds):\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e1256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:03,  5.08it/s]/tmp/ipykernel_22089/234230906.py:4: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_annotations = pd.read_csv(\n",
      "24it [00:06,  5.08it/s]/tmp/ipykernel_22089/234230906.py:4: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_annotations = pd.read_csv(\n",
      "26it [00:07,  3.54it/s]/tmp/ipykernel_22089/234230906.py:4: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_annotations = pd.read_csv(\n",
      "27it [00:08,  3.35it/s]/tmp/ipykernel_22089/234230906.py:4: DtypeWarning: Columns (2,3,4,5,6,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_annotations = pd.read_csv(\n",
      "33it [00:11,  3.10it/s]/tmp/ipykernel_22089/234230906.py:4: DtypeWarning: Columns (10,11,13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_annotations = pd.read_csv(\n",
      "50it [00:13,  3.75it/s]\n",
      "/tmp/ipykernel_22089/234230906.py:27: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_annotations = pd.concat(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (seconds): 15.807878017425537\n"
     ]
    }
   ],
   "source": [
    "# start_time = time.time()\n",
    "# df_annotation_list = []\n",
    "# for i, source_id in tqdm.tqdm(enumerate(source_ids_start)):\n",
    "#     df_annotations = pd.read_csv(\n",
    "#         f\"s3://{source_bucket}/{source_s3_prefix}/s{source_id}/annotations.csv\"\n",
    "#     )\n",
    "#     df_images = pd.read_csv(\n",
    "#         f\"s3://{source_bucket}/{source_s3_prefix}/s{source_id}/image_list.csv\"  # Perhaps this is unnecessary and can just use tha annotations as in Mermaid\n",
    "#     )\n",
    "#     df_images[\"Name\"] = df_images[\"Name\"].apply(\n",
    "#         lambda x: x.replace(\" - Confirmed\", \"\")\n",
    "#     )\n",
    "#     df_images[\"image_id\"] = df_images[\"Image Page\"].apply(\n",
    "#         lambda x: x.replace(\"/image/\", \"\").replace(\"/view/\", \"\")\n",
    "#     )\n",
    "#     df_annotations = pd.merge(\n",
    "#         df_annotations,\n",
    "#         df_images,\n",
    "#         left_on=\"Name\",\n",
    "#         right_on=\"Name\",\n",
    "#         how=\"left\",\n",
    "#         suffixes=(\"\", \"_y\"),\n",
    "#     )\n",
    "#     df_annotations[\"source_id\"] = source_id\n",
    "#     df_annotation_list.append(df_annotations)\n",
    "\n",
    "# df_annotations = pd.concat(\n",
    "#     df_annotation_list, ignore_index=True\n",
    "# )\n",
    "# end_time = time.time()\n",
    "# print(\"Time taken (seconds):\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68f40cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1427it [04:13,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (seconds): 257.25816226005554\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df_annotation_list = []\n",
    "for i, source_id in tqdm.tqdm(enumerate(source_ids)):\n",
    "    df_annotations = pd.read_csv(\n",
    "        f\"s3://{source_bucket}/{source_s3_prefix}/s{source_id}/annotations.csv\", low_memory=False\n",
    "    )\n",
    "    df_images = pd.read_csv(\n",
    "        f\"s3://{source_bucket}/{source_s3_prefix}/s{source_id}/image_list.csv\", low_memory=False  # Perhaps this is unnecessary and can just use tha annotations as in Mermaid\n",
    "    )\n",
    "    df_images[\"Name\"] = df_images[\"Name\"].apply(\n",
    "        lambda x: x.replace(\" - Confirmed\", \"\")\n",
    "    )\n",
    "    df_images[\"image_id\"] = df_images[\"Image Page\"].apply(\n",
    "        lambda x: x.replace(\"/image/\", \"\").replace(\"/view/\", \"\")\n",
    "    )\n",
    "    df_annotations = pd.merge(\n",
    "        df_annotations,\n",
    "        df_images,\n",
    "        left_on=\"Name\",\n",
    "        right_on=\"Name\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_y\"),\n",
    "    )\n",
    "    df_annotations[\"source_id\"] = source_id\n",
    "    df_annotation_list.append(df_annotations[[\"source_id\", \"image_id\", \"Row\", \"Column\", \"Label ID\"]])\n",
    "\n",
    "df_annotations = pd.concat(\n",
    "    df_annotation_list, ignore_index=True\n",
    ")\n",
    "end_time = time.time()\n",
    "print(\"Time taken (seconds):\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc383879",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotations = df_annotations.rename(\n",
    "    columns={\n",
    "        \"image_id\": \"image_id\",\n",
    "        \"Row\": \"row\",\n",
    "        \"Column\": \"col\",\n",
    "        \"Label ID\": \"coralnet_id\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a09e3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21326860, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f1a5cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2886506663635373"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations.memory_usage(deep=True).sum() / (1024 ** 3)  # in GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af6ba45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coralnet-public-images'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_s3_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbbef394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev-datamermaid-sm-sources'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05622a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "path = f\"s3://{source_bucket}/coralnet_annotations_30112025.parquet\"\n",
    "\n",
    "df_annotations.to_parquet(path, engine=\"pyarrow\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e19e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "except Exception:\n",
    "    # fallback: open S3 file with s3fs and write to file-like object\n",
    "\n",
    "    fs = s3fs.S3FileSystem()\n",
    "    with fs.open(path, \"wb\") as f:\n",
    "        df_annotations.to_parquet(f, engine=\"pyarrow\", index=False)\n",
    "\n",
    "print(\"Saved:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5569519c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca79700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f73fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(session=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63b045b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Site</th>\n",
       "      <th>Depth</th>\n",
       "      <th>Transect</th>\n",
       "      <th>Metermark</th>\n",
       "      <th>Aux5</th>\n",
       "      <th>Height (cm)</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Depth.1</th>\n",
       "      <th>Camera</th>\n",
       "      <th>Photographer</th>\n",
       "      <th>Water quality</th>\n",
       "      <th>Strobes</th>\n",
       "      <th>Framing gear used</th>\n",
       "      <th>White balance card</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Row</th>\n",
       "      <th>Column</th>\n",
       "      <th>Label code</th>\n",
       "      <th>Label ID</th>\n",
       "      <th>Annotator</th>\n",
       "      <th>Date annotated</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "      <td>float64</td>\n",
       "      <td>string</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>string</td>\n",
       "      <td>int64</td>\n",
       "      <td>string</td>\n",
       "      <td>string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<div>Dask Name: to_string_dtype, 2 expressions</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                 Name    Date    Site    Depth Transect Metermark     Aux5 Height (cm) Latitude Longitude  Depth.1   Camera Photographer Water quality  Strobes Framing gear used White balance card Comments    Row Column Label code Label ID Annotator Date annotated\n",
       "npartitions=1                                                                                                                                                                                                                                                           \n",
       "               string  string  string  float64   string     int64  float64       int64  float64   float64  float64  float64      float64       float64  float64           float64            float64  float64  int64  int64     string    int64    string         string\n",
       "                  ...     ...     ...      ...      ...       ...      ...         ...      ...       ...      ...      ...          ...           ...      ...               ...                ...      ...    ...    ...        ...      ...       ...            ...\n",
       "Dask Name: to_string_dtype, 2 expressions\n",
       "Expr=ArrowStringConversion(frame=FromMapProjectable(b7ff2c0))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4c03fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:05,  1.99it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+--------------+--------+----------+\n| Column       | Found  | Expected |\n+--------------+--------+----------+\n| Photographer | object | float64  |\n+--------------+--------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- Photographer\n  ValueError(\"could not convert string to float: 'KO'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'Photographer': 'object'}\n\nto the call to `read_csv`/`read_table`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m df_annotation_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, source_id \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28menumerate\u001b[39m(source_ids_start)):\n\u001b[1;32m      4\u001b[0m     df_annotations \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msource_bucket\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msource_s3_prefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/s\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msource_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/annotations.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m----> 6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     df_images \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_bucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_s3_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/s\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/image_list.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Perhaps this is unnecessary and can just use tha annotations as in Mermaid\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     )\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m     10\u001b[0m     df_images[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df_images[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m - Confirmed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/dask/base.py:373\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/dask/base.py:681\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m     expr \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39moptimize()\n\u001b[1;32m    679\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(flatten(expr\u001b[38;5;241m.\u001b[39m__dask_keys__()))\n\u001b[0;32m--> 681\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack(results)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/dask/dataframe/io/csv.py:351\u001b[0m, in \u001b[0;36m_read_csv\u001b[0;34m(block, part, columns, reader, header, dtypes, head, colname, full_columns, enforce, kwargs, blocksize)\u001b[0m\n\u001b[1;32m    348\u001b[0m         rest_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _columns\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# Call `pandas_read_text`\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_read_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrest_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project_after_read:\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[columns]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/dask/dataframe/io/csv.py:79\u001b[0m, in \u001b[0;36mpandas_read_text\u001b[0;34m(reader, b, header, kwargs, dtypes, columns, write_header, enforce, path)\u001b[0m\n\u001b[1;32m     77\u001b[0m df \u001b[38;5;241m=\u001b[39m reader(bio, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtypes:\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mcoerce_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enforce \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m(columns)):\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns, columns)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/dask/dataframe/io/csv.py:180\u001b[0m, in \u001b[0;36mcoerce_dtypes\u001b[0;34m(df, dtypes)\u001b[0m\n\u001b[1;32m    176\u001b[0m rule \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m61\u001b[39m)\n\u001b[1;32m    177\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatched dtypes found in `pd.read_csv`/`pd.read_table`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    178\u001b[0m     rule\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, [dtype_msg, date_msg]))\n\u001b[1;32m    179\u001b[0m )\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+--------------+--------+----------+\n| Column       | Found  | Expected |\n+--------------+--------+----------+\n| Photographer | object | float64  |\n+--------------+--------+----------+\n\nThe following columns also raised exceptions on conversion:\n\n- Photographer\n  ValueError(\"could not convert string to float: 'KO'\")\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'Photographer': 'object'}\n\nto the call to `read_csv`/`read_table`."
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df_annotation_list = []\n",
    "for i, source_id in tqdm.tqdm(enumerate(source_ids_start)):\n",
    "    df_annotations = dd.read_csv(\n",
    "        f\"s3://{source_bucket}/{source_s3_prefix}/s{source_id}/annotations.csv\"\n",
    "    ).compute()\n",
    "    df_images = dd.read_csv(\n",
    "        f\"s3://{source_bucket}/{source_s3_prefix}/s{source_id}/image_list.csv\"  # Perhaps this is unnecessary and can just use tha annotations as in Mermaid\n",
    "    ).compute()\n",
    "    df_images[\"Name\"] = df_images[\"Name\"].apply(\n",
    "        lambda x: x.replace(\" - Confirmed\", \"\")\n",
    "    )\n",
    "    df_images[\"image_id\"] = df_images[\"Image Page\"].apply(\n",
    "        lambda x: x.replace(\"/image/\", \"\").replace(\"/view/\", \"\")\n",
    "    )\n",
    "    df_annotations = dd.merge(\n",
    "        df_annotations,\n",
    "        df_images,\n",
    "        left_on=\"Name\",\n",
    "        right_on=\"Name\",\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_y\"),\n",
    "    )\n",
    "    df_annotations[\"source_id\"] = source_id\n",
    "    df_annotation_list.append(df_annotations[[\"Image Page\", \"Row\", \"Column\", \"Label ID\"]])\n",
    "\n",
    "df_annotations = dd.concat(\n",
    "    df_annotation_list, ignore_index=True\n",
    ")\n",
    "\n",
    "# convert Dask DataFrame to pandas DataFrame\n",
    "df_annotations = df_annotations\n",
    "\n",
    "# create image_id from \"Image Page\" (same logic as earlier)\n",
    "df_annotations[\"image_id\"] = (\n",
    "    df_annotations[\"Image Page\"]\n",
    "    .str.replace(\"/image/\", \"\", regex=False)\n",
    "    .str.replace(\"/view/\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "# keep the relevant columns (optional)\n",
    "df_annotations = df_annotations[[\"image_id\", \"Row\", \"Column\", \"Label ID\"]]\n",
    "end_time = time.time()\n",
    "print(\"Time taken (seconds):\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a050f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "delayed_dfs = [delayed(pd.read_csv)(fs.open(p, mode='rb')) for p in paths]\n",
    "ddf = dd.from_delayed(delayed_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba84f433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Name",
         "rawType": "string",
         "type": "string"
        },
        {
         "name": "Date",
         "rawType": "string",
         "type": "string"
        },
        {
         "name": "Site",
         "rawType": "string",
         "type": "string"
        },
        {
         "name": "Transect Depth",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Transect Number",
         "rawType": "string",
         "type": "string"
        },
        {
         "name": "Photo Number",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Aux5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Height (cm)",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Latitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Longitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Depth",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Camera",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Photographer",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Water quality",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Strobes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Framing gear used",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "White balance card",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Comments",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Row",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Column",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Label code",
         "rawType": "string",
         "type": "string"
        },
        {
         "name": "Label ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Annotator",
         "rawType": "string",
         "type": "string"
        },
        {
         "name": "Date annotated",
         "rawType": "string",
         "type": "string"
        }
       ],
       "ref": "7fa522f3-f153-49a7-b012-7d193f4bcb2d",
       "rows": [
        [
         "0",
         "APT_100_D_01_2013-07-24.jpg",
         "2013-07-24",
         "APT",
         "100.0",
         "D",
         "1.0",
         null,
         "50",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "357",
         "371",
         "Turf",
         "82",
         "mbogeberg",
         "2014-02-12 21:42:05+00:00"
        ],
        [
         "1",
         "APT_100_D_01_2013-07-24.jpg",
         "2013-07-24",
         "APT",
         "100.0",
         "D",
         "1.0",
         null,
         "50",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "569",
         "613",
         "Turf",
         "82",
         "mbogeberg",
         "2014-02-12 21:42:05+00:00"
        ],
        [
         "2",
         "APT_100_D_01_2013-07-24.jpg",
         "2013-07-24",
         "APT",
         "100.0",
         "D",
         "1.0",
         null,
         "50",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "480",
         "392",
         "Turf",
         "82",
         "mbogeberg",
         "2014-02-12 21:42:05+00:00"
        ],
        [
         "3",
         "APT_100_D_01_2013-07-24.jpg",
         "2013-07-24",
         "APT",
         "100.0",
         "D",
         "1.0",
         null,
         "50",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "418",
         "202",
         "Turf",
         "82",
         "mbogeberg",
         "2014-02-12 21:42:05+00:00"
        ],
        [
         "4",
         "APT_100_D_01_2013-07-24.jpg",
         "2013-07-24",
         "APT",
         "100.0",
         "D",
         "1.0",
         null,
         "50",
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         null,
         "487",
         "599",
         "Turf",
         "82",
         "mbogeberg",
         "2014-02-12 21:42:05+00:00"
        ]
       ],
       "shape": {
        "columns": 24,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Site</th>\n",
       "      <th>Transect Depth</th>\n",
       "      <th>Transect Number</th>\n",
       "      <th>Photo Number</th>\n",
       "      <th>Aux5</th>\n",
       "      <th>Height (cm)</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>...</th>\n",
       "      <th>Strobes</th>\n",
       "      <th>Framing gear used</th>\n",
       "      <th>White balance card</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Row</th>\n",
       "      <th>Column</th>\n",
       "      <th>Label code</th>\n",
       "      <th>Label ID</th>\n",
       "      <th>Annotator</th>\n",
       "      <th>Date annotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>APT_100_D_01_2013-07-24.jpg</td>\n",
       "      <td>2013-07-24</td>\n",
       "      <td>APT</td>\n",
       "      <td>100.0</td>\n",
       "      <td>D</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>357</td>\n",
       "      <td>371</td>\n",
       "      <td>Turf</td>\n",
       "      <td>82</td>\n",
       "      <td>mbogeberg</td>\n",
       "      <td>2014-02-12 21:42:05+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APT_100_D_01_2013-07-24.jpg</td>\n",
       "      <td>2013-07-24</td>\n",
       "      <td>APT</td>\n",
       "      <td>100.0</td>\n",
       "      <td>D</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>569</td>\n",
       "      <td>613</td>\n",
       "      <td>Turf</td>\n",
       "      <td>82</td>\n",
       "      <td>mbogeberg</td>\n",
       "      <td>2014-02-12 21:42:05+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>APT_100_D_01_2013-07-24.jpg</td>\n",
       "      <td>2013-07-24</td>\n",
       "      <td>APT</td>\n",
       "      <td>100.0</td>\n",
       "      <td>D</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>480</td>\n",
       "      <td>392</td>\n",
       "      <td>Turf</td>\n",
       "      <td>82</td>\n",
       "      <td>mbogeberg</td>\n",
       "      <td>2014-02-12 21:42:05+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>APT_100_D_01_2013-07-24.jpg</td>\n",
       "      <td>2013-07-24</td>\n",
       "      <td>APT</td>\n",
       "      <td>100.0</td>\n",
       "      <td>D</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>418</td>\n",
       "      <td>202</td>\n",
       "      <td>Turf</td>\n",
       "      <td>82</td>\n",
       "      <td>mbogeberg</td>\n",
       "      <td>2014-02-12 21:42:05+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>APT_100_D_01_2013-07-24.jpg</td>\n",
       "      <td>2013-07-24</td>\n",
       "      <td>APT</td>\n",
       "      <td>100.0</td>\n",
       "      <td>D</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>487</td>\n",
       "      <td>599</td>\n",
       "      <td>Turf</td>\n",
       "      <td>82</td>\n",
       "      <td>mbogeberg</td>\n",
       "      <td>2014-02-12 21:42:05+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Name        Date Site  Transect Depth  \\\n",
       "0  APT_100_D_01_2013-07-24.jpg  2013-07-24  APT           100.0   \n",
       "1  APT_100_D_01_2013-07-24.jpg  2013-07-24  APT           100.0   \n",
       "2  APT_100_D_01_2013-07-24.jpg  2013-07-24  APT           100.0   \n",
       "3  APT_100_D_01_2013-07-24.jpg  2013-07-24  APT           100.0   \n",
       "4  APT_100_D_01_2013-07-24.jpg  2013-07-24  APT           100.0   \n",
       "\n",
       "  Transect Number  Photo Number  Aux5  Height (cm)  Latitude  Longitude  ...  \\\n",
       "0               D           1.0   NaN           50       NaN        NaN  ...   \n",
       "1               D           1.0   NaN           50       NaN        NaN  ...   \n",
       "2               D           1.0   NaN           50       NaN        NaN  ...   \n",
       "3               D           1.0   NaN           50       NaN        NaN  ...   \n",
       "4               D           1.0   NaN           50       NaN        NaN  ...   \n",
       "\n",
       "   Strobes  Framing gear used  White balance card  Comments  Row  Column  \\\n",
       "0      NaN                NaN                 NaN       NaN  357     371   \n",
       "1      NaN                NaN                 NaN       NaN  569     613   \n",
       "2      NaN                NaN                 NaN       NaN  480     392   \n",
       "3      NaN                NaN                 NaN       NaN  418     202   \n",
       "4      NaN                NaN                 NaN       NaN  487     599   \n",
       "\n",
       "   Label code  Label ID  Annotator             Date annotated  \n",
       "0        Turf        82  mbogeberg  2014-02-12 21:42:05+00:00  \n",
       "1        Turf        82  mbogeberg  2014-02-12 21:42:05+00:00  \n",
       "2        Turf        82  mbogeberg  2014-02-12 21:42:05+00:00  \n",
       "3        Turf        82  mbogeberg  2014-02-12 21:42:05+00:00  \n",
       "4        Turf        82  mbogeberg  2014-02-12 21:42:05+00:00  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "bucket = \"dev-datamermaid-sm-sources\"\n",
    "source_id = 109\n",
    "prefix = \"coralnet-public-images\"\n",
    "\n",
    "path = f\"s3://{bucket}/{prefix}/s{source_id}/annotations.csv\"\n",
    "ddf = dd.read_csv(path)   # blocksize controls partitioning\n",
    "# inspect\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a23930f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dask.dataframe.dask_expr._collection.DataFrame"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c975a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an S3FileSystem instance with the credentials from secrets.json\n",
    "s3 = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e0168db",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/sagemaker-user/mermaid-segmentation/nbs/coralnet-public-images/s1301/annotations.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/dask/backends.py:140\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/dask/dataframe/io/csv.py:731\u001b[0m, in \u001b[0;36mmake_reader.<locals>.read\u001b[0;34m(urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread\u001b[39m(\n\u001b[1;32m    719\u001b[0m     urlpath,\n\u001b[1;32m    720\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    730\u001b[0m ):\n\u001b[0;32m--> 731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m        \u001b[49m\u001b[43murlpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43massume_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massume_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_path_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_path_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/dask/dataframe/io/csv.py:524\u001b[0m, in \u001b[0;36mread_pandas\u001b[0;34m(reader, urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    523\u001b[0m     sample \u001b[38;5;241m=\u001b[39m blocksize\n\u001b[0;32m--> 524\u001b[0m b_out \u001b[38;5;241m=\u001b[39m \u001b[43mread_bytes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43murlpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb_lineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_path_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_path_column:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/dask/bytes/core.py:111\u001b[0m, in \u001b[0;36mread_bytes\u001b[0;34m(urlpath, delimiter, not_zero, blocksize, sample, compression, include_path, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot do chunked reads on compressed files. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo read, set blocksize=None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m     )\n\u001b[0;32m--> 111\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/fsspec/implementations/local.py:99\u001b[0m, in \u001b[0;36mLocalFileSystem.info\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strip_protocol(path)\n\u001b[0;32m---> 99\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m link \u001b[38;5;241m=\u001b[39m stat\u001b[38;5;241m.\u001b[39mS_ISLNK(out\u001b[38;5;241m.\u001b[39mst_mode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sagemaker-user/mermaid-segmentation/nbs/coralnet-public-images/s1301/annotations.csv'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_tmp \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msource_s3_prefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/s\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msource_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/annotations.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/dask/backends.py:151\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: [Errno 2] No such file or directory: '/home/sagemaker-user/mermaid-segmentation/nbs/coralnet-public-images/s1301/annotations.csv'"
     ]
    }
   ],
   "source": [
    "df_tmp = dd.read_csv(f\"{source_s3_prefix}/s{source_id}/annotations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8094a5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "\n",
    "\n",
    "    # Use s3 to access S3 objects\n",
    "    bucket_name = 'coralnet-mermaid-share'\n",
    "    missing_sources = []\n",
    "\n",
    "    for source in chosen_sources:\n",
    "        if s3.exists(f'{bucket_name}/{source}'):\n",
    "            \n",
    "            print(f\"{source} exists in the bucket.\")\n",
    "            df = dd.read_csv(f's3://{bucket_name}/{source}', storage_options={'key': secrets['AWS_ACCESS_KEY_ID'], 'secret': secrets['AWS_SECRET_ACCESS_KEY']})\n",
    "\n",
    "            df['source_id'] = source.split('/')[1]\n",
    "            dfs.append(df)\n",
    "        else:\n",
    "            missing_sources.append(source)\n",
    "            print(f\"{source} does not exist in the bucket.\")\n",
    "except (ClientError, BotoCoreError) as e:\n",
    "    print(f\"An AWS error occurred: {e}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error reading secrets.json: {e}\")\n",
    "except IOError as e:\n",
    "    print(f\"File error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
